\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{url}
\usepackage[square]{natbib}
\usepackage{booktabs, caption, makecell}


\title{\Large TITLE}
\author{\large Suyoung Park$^1$, Alexander E. Lipka$^2$, Daniel J. Eck$^1$\\[1em]
\small 1. Department of Statistics, University of Illinois Urbana-Champaign \\
\small 2. Department of Crop Sciences, University of Illinois Urbana-Champaign}
\date{\normalsize August, 2021}

\begin{document}
\maketitle
\begin{abstract}
Quantitative genetics methodology has facilitated advances in the basic understanding of which genes underlie agronomically important quantitative traits in crop sciences. Although less commonplace than quantitative traits, agronomically important binary traits do occur in such genomics studies. The logistic regression model is a widely used model for analyses involving binary traits. This model is specifically constructed for such analyses. That being said, this model breaks down when there is separation in the data. Separation occurs when there exists a hyperplane in the covariate space such that deterministic outcomes are observed on at least one side of this hyperplane. Data separation is especially prevalent in applications where the number of predictors under investigation is near the sample size. In this study we motivate a logistic regression model that is robust to separation, and we develop a novel prediction procedure for this robust logistic regression model that is appropriate when separation exists. We compare our robust logistic regression model to existing approaches. Previously existing approaches treat separation as a modeling shortcoming and not an antagonistic data configuration. They therefore change the modeling paradigm to account for problematic separation while we accommodate separation within the standard logistic regression maximum likelihood estimation paradigm. Our comparisons are conducted on several didactic examples and a genomics study on the kernel color in maize. We find that our robust logistic regression model provides superior statistical inferences while maintaining competitive predictive performance. Our results are fully reducible in an accompanying technical report.
%In quantitative genetics, an analysis of binary traits such as the color of kernels in maize is the promising topic that can have a significant impact on agronomic studies. However, most quantitative genetics approaches which were devised for continuous quantitative traits do not guarantee the state-of-the-art performance when we apply these methods to the binary traits because of the different nature of the binary variable. Therefore, the logistic regression model is widely considered for the binary outcome variable. The logistic model, however, fails when one or more covariates can perfectly predict the binary traits. In this study, we propose the robust logistic regression model that provides the valid inference and prediction invariant to the configuration of data. Our model solved the conventional issue within an original model unlike popular models in the literature that changed the problem settings of the original model. As a result, our model showed the most accurate inference with the highest in-sample accuracy and smallest confidence interval in all datasets that we considered. Moreover, it demonstrated a comparable prediction performance. Our new method provides the most preferable approach to handle the issue from the logistic regression model considering overall performance. We expect our method is especially useful for the study of binary traits in quantitative genetics field because a larger number of covariates with small observations usually raises the issue in the logistic regression. Furthermore, our model can be applicable to any other discipline where perform the binary classification using the logistic model.
\end{abstract}
\smallskip
\noindent \textbf{Keywords:} Logistic regression; Complete separation; Quantitative genetics

\section{Introduction}

The application of quantitative genetics approaches to crops has facilitated advances in the basic understanding of which genes underlie agronomically important traits, and has enabled the use of genome-wide markers to accelerate genetic gain. For example, the use of multivariate statistical models in genome-wide association studies has provided insight into the role of pleiotropy in the genetic architecture of leaf and infloresence-related traits in maize \citep{rice2020multi}. Similarly, multi-kernel genomic prediction (GP) models that include environmental covariate information have made it possible to accurately predict genomic estimated breeding values (GEBVs) for grain yield in wheat in specific environments \citep{jarquin2014reaction}. Although less commonplace than quantitative traits, agronomically important binary traits do occur. For example the color of kernels in maize is often dichotomized into a binary trait (white versus yellow; \citep{Romay2013}), and breeding for kernel color is a critical step for increasing bioavailablity of provitamin A carotenoids in maize grain \citep{chandler2013genetic, harjes2008natural}. Thus, the  application of quantitative genetic analysis to binary traits has great potential to have a meaningful impact on future agronomic efforts. However, a major setback is that some of the most widely-used quantitative genetics approaches in agronomy do not account for the dichotomous configuration of a binary's trait. Consequently, direct application of state-of-the-art quantitative genetic approaches to study binary traits could result in negative statistical ramifications, including inadequate control of inflation of test statistics due to subpopulation structure (as shown in, e.g. \citep{shenstone2018assessment}).

The logistic regression model is one of the most common statistical models in settings where a binary outcome variable depends on a set of covariates. This model breaks down when there is separation in the data. Separation occurs when there exists a hyperplane in the covariate space such that deterministic outcomes are observed on at least one side of this hyperplane. When separation is present logistic model coefficient estimates are not finite (or unstable). Therefore, any interpretations or conducting significance tests on coefficients is meaningless. Moreover, common statistical software does not diagnose this issue or provide remedies \citep{Eck2021}. Data separation is especially prevalent when the number of predictors is near the sample size.


The easiest way to deal with data separation, when it is detected, is to remove the problematic covariates. However, this na\"ive approach often leads us to get rid of the highly correlated covariates with the outcome variable \citep{Zorn2005}. Alternatively, \cite{Heinze2002} use the Firth's penalized maximum likelihood estimation to reduce the bias of maximum likelihood estimator to obtain the finite parameter estimates. \cite{Kosmidis2009} then generalize this method for the nonlinear exponential family. These bias reduction methods enable one to estimate coefficients when the coefficients of problematic covariates are at infinity. Additionally, many have proposed a Bayesian framework to handle the estimation problems that arise from separation \citep{Heinze2002, Dunson_2006, Genkin_2007, Gelman2008}. \citeauthor{Heinze2002}'s method can be seen as the application of the Jeffrey's invariant prior. \cite{Dunson_2006} use the mixture prior distributions for the logistic model with large number of covariates and \cite{Genkin_2007} consider the Laplace prior distribution. \cite{Gelman2008} suggest the Cauchy distribution with center 0 and scale 2.5 as the default choice, and this method shows faster and better performance in the prediction in comparison to the other methods. 

Both bias reduction and Bayesian approaches handle the separation issue by switching the modeling paradigm to accommodate problematic data rather than solving the issue within the original model. \cite{Geyer2009} developed methodology for directly finding the MLE when problematic separation exists and the traditional MLE calculations do not converge. This method requires a massive computation cost which makes it time consuming to apply in practice. \cite{Eck2021} proposed new, faster and scalable methodology to find the MLE in the completion when MLE does not exist. \citeauthor{Eck2021}'s method is implemented in the R package \texttt{glmdr}, software which detects and remedies separation in logistic regression. In this study we propose the prediction framework in the \cite{Eck2021}'s method. Considering the important role of statistical model is often to preform the inference and prediction, our work can make \cite{Eck2021}'s method more practical and useful to use when the separation issue presents.

In this study we motivate the logistic regression model for applications in binary outcome regression, describe the problem of separation in the data, and compare different techniques (Bayesian, penalized likelihood, and maximum likelihood estimation) for handling separation on several didactic datasets and practical datasets in biostatistics and genetics. We assess performance of these techniques on their inferential and predictive ability. Because the MLE asymptotically achieves the Cram{\'e}r-Rao lower bound, we expect the MLE technique in \cite{Eck2021} to yield the tightest inferences among all techniques under consideration. This finding is confirmed in all datasets that we considered. We develop a novel prediction procedure within the methodological context of \cite{Eck2021} to facilitate prediction when there exists separation in the data. We expect our developed method and the other considered methods to exhibit even predictive performance with a computational edge towards the Bayesian techniques that we considered. While the method of \cite{Eck2021} is far more computationally convenient than that of \cite{Geyer2009} it is still rather involved when adapted for prediction. Ultimately we want to develop the methodology in \cite{Eck2021} for genomic prediction when there may be far more predictors than cases. The prediction procedures developed here are an important step in that direction.

\section{Materials and Methods}

\subsection{Logistic Regression}
The logistic regression is the special case of the generalized linear model which the outcome variable follows Bernoulli distribution (i.e., $y \in \{0,1\}$) \citep{GLM_paper}. By convention, we encode 1 as a ``success" and 0 as a ``failure." In logistic regression the conditional success probability at a particular $x$ is modeled as
\begin{equation}\label{logistic_model}
\text{Pr}(Y = 1 | X = x) = \frac{\exp{(x^{T}\beta)}}{1+\exp{(x^{T}\beta)}} = p_x,
\end{equation}
where $\beta$ is an unknown canonical parameter vector (coefficient vector), $X$ and $Y$ are the covariate and outcome random variables, and $x$ is an observed value.

From the linear regression's point of view, this logistic regression is equivalent to:
\begin{equation}\label{link_function}
g(p_x) = \log\left(\frac{p_x}{1-p_x}\right) = x^T\beta
\end{equation}
where $g(x) = \log(\frac{x}{1-x})$ is a logit link (log-odds ratio).

Therefore, as in classical ordinary least squares (OLS) regression, we can estimate model parameters using maximum likelihood estimation. Statistical inferences about model parameters can be obtained from estimates of the Fisher information. Unlike in OLS regression, estimates for $\hat\beta$ are not given in closed form. The log-likelihood function for the logistic regression model is
\begin{equation}\label{likelihood}
\log L(\beta|Y) = \sum_{i=1}^{n} y_i\log{(p_{x_i})} + (1-y_i)\log{(1-p_{x_i})},
\end{equation}
one then obtains $\hat\beta$ by solving the score function equation
\begin{equation}\label{logit_Score}
\frac{\partial \log{L(\beta|Y)}}{\partial \beta} = 
\sum_{i=1}^{N} (y_i - \log{(p_{x_i})}) x_i^T =
\sum_{i=1}^{N} [y_i + \log{(1+\exp{(-x_i^T\beta)})}] = 0.
\end{equation}
Conventional softwares finds $\hat\beta$ through Fisher-scoring or iteratively reweighted least squares algorithms \citep[Chapter 4]{agresti_categorical_2013}. We then obtain inferences using an estimate of the Fisher information matrix evaluated at the MLE solution $\hat{\beta}$
\begin{equation}\label{logit_Fisher}
\widehat{\text{Var}(\beta)} = [I(\hat\beta)]^{-1} = \left(-E\left[\frac{\partial^2 \log{L(\beta|Y)}}{\partial \beta_i \partial \beta_j}\right]\right)^{-1}\Big\vert_{\beta = \hat\beta}.
\end{equation}
Conventional software provides \eqref{logit_Fisher}.

\subsection{Complete Separation}
\label{sec:complete_separation}

Traditional maximum likelihood estimation for logistic regression does not work well when there is complete or quasi-complete separation in the data, a problem that is widespread in applications \citep{Geyer2009}. \cite{agresti_categorical_2013} defines complete separation when there exists a vector $b$ such that 

\begin{equation}\label{definition_complete_separation}
\begin{split}
x_{i}^{T} b &> 0 \text{ whenever } y_i = 1, \\
x_{i}^{T} b &< 0 \text{ whenever } y_i = 0.
\end{split}
\end{equation}
That is, complete separation occurs when the one or more covariates can perfectly predict the outcome variable \citep{AlbertAnderson1984}. For example, as shown in the Figure~\ref{Fig:Complete_Separation_Example}, consider the following case that when $x$ is less than 50, all corresponding $y$ are $0$ and when $x$ is greater than 50, all corresponding $y$ are $1$. Suppose we are interested in a simple logistic regression model $x_i^T = [1, z_i]$. Then this data is completely separated with $b = [-50, 1]^T.$ Moreover, we have $\hat{p}_x = 0$ for $z < 50$ and $\hat{p}_x = 1$ for $z > 50$.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{Figures/Agresti_Complete_Separation.png}
    \caption{Example of complete separation from Section 6.5.1 of \cite{agresti_categorical_2013}. The conventional MLE of a logistic model does not exist.}\label{Fig:Complete_Separation_Example}
\end{figure}

When there is complete separation, the parameter estimates $\hat\beta$ are ``at infinity,'' the iteration based estimation algorithms provide a sequence of estimates that goes to infinity, and the log likelihood becomes flat when evaluated along this sequence. The left panel of Figure \ref{Fig:asymptotes} shows the log likelihood of logistic model for this example with different working estimate from $\texttt{glm}$ function in R. We can see that each iteration, norm of $\beta$ becomes larger and asymptote of the log likelihood value goes to infinity. The right panel of Figure \ref{Fig:asymptotes} is the zoomed part of the left panel of Figure \ref{Fig:asymptotes} where the log of norm of working estimates is between 4.5 and 5. It displays the log likelihood value still approaches near zero although the left panel of Figure \ref{Fig:asymptotes} looks flat in the same region. In complete separation, the usual statistical inference is not valid. The standard errors of predicted probabilities of success are very small, which leads to extremely narrow confidence intervals for each observation. Unfortunately, none of common statistical software such as R, SAS and Python can handle the separation issue properly and uninformed users sometimes uses the wrong model without knowing it \citep{R_software, SAS_software, Python_software}. The \texttt{glmdr} software package \citep{glmdr_package} is designed to provide users with a description of the complete separation problem, and provide statistical inferences when it occurs.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{Figures/asymptotes.png}
    \caption{\textbf{Left panel}: Log likelihood values of logistic model at different working estimates. Blue dot represents the log likelihood value at each iteration. \textbf{Right panel:} Zoom in view of a log likelihood values of logistic model where log of norm of working estimates lie between 4.5 and 5.}\label{Fig:asymptotes}
\end{figure}

Quasi-complete separation is another case of separation that there are both a success and a failure on the hyperplane that separates the successes from the failures \citep{lesaffre_albert_1989}. For instance, we can consider additional two points that $z = 50$ with $y=1$ and $y=0$ to the previous complete separation example. That is, we have $y_i = 0$ for $z \leq 50$ and $y_i =1$ for $z \geq 50$. In this case, the maximized log likelihood is always negative and we experience same phenomenon as the complete separation case.

\subsection{Mean-value Parameters}

The parameter of primary interest is often the mean-value parameter on the scale of the outcome variable. This is the expected outcome expressed as a function of covariates. In the logistic regression model the mean-value parameter is the conditional success probability $p_x$ at some particular $x$, and, unlike in linear regression, this parameter is not easily interpreted from $\beta$. Furthermore, the natural constraints on a conditional probability corresponding to a binary outcome variable require an alteration to the linear model. 

In linear regression, we can easily obtain $\text{E}(Y|X=x)$ from $\beta$ since $\text{E}(Y|X=x) = x^{T}\beta$. Plugging in $\hat{\beta}$ produces the MLE for this expectation $\widehat{\text{E}}(Y|X=x) = x^{T}\hat\beta$ with $x$ fixed. On the other hand, in the logistic model, $\text{E}(Y|X=x) = \text{Pr}(Y = 1 | X=x)$ where $\log(\frac{p_{x}}{1-p_{x}}) = x^T\beta$. Thus, $\beta$ does not offer an easy interpretation about changes in the expected outcome as the covariates change, and it is therefore less useful as a parameter for understanding how $p_x$ changes with $x$. The mean-value parametrization is the primary parameter of interest in both regression contexts, but in linear regression the mean-value parameter and $\beta$ are interchangeable.

Another benefit of the mean-value parameterization over $\beta$ in the logistic regression model is when complete separation exists. When complete separation exists $\beta$ is estimated to be at infinity while $p_x$ is estimated to be 0 or 1. We discuss complete separation and methods which address it in the next Section.

\subsection{One-Sided Confidence Interval}

We use one-sided confidence intervals for the logistic model's mean-value parameters to explain the uncertainty of estimation in the presence of separation. The original concept can be found in Section 3.16 of Geyer's paper [\citeyear{Geyer2009}] and implementation details can be found in Section 4.3 of \cite{Eck2021}. These one-sided intervals are specifically tailored to handle separation and they are what form our robust logistic regression model.

For completeness we briefly explain how we construct these one-sided confidence interval for mean-value parameters. One endpoint of the one-sided interval is constrained to be the observed outcome variable (i.e., lower bound if $y_i=0$ and upper bound if $y_i=1$), and the other endpoint is obtained by solving the optimization problem:

\begin{equation} \label{CI_logistic}
\begin{split}
    \text{minimize } & \quad -\theta_k \\
    \text{subject to } & \sum_{i \in I}[y_{i} \log{(p_{x_i})} + (1 - y_i) \log{(1-p_{x_i}})] - \log{(\alpha) \geq 0},
\end{split}
\end{equation}
where $\theta_k = x_k^T\beta$ for any $k \in I$, $\textit{I}$ is a index of problematic points that cause the separation, $p$ is a mean-value parameter, and $\alpha$ is a significance level. For example, Figure \ref{Fig:One_sided_CI} shows the one-sided confidence interval for the complete separation example we discussed in Section \ref{sec:complete_separation}. We can see the confidence interval increases as $z$ increases until $z = 40$ then it starts to decrease as $z$ increases from $z = 60$. Also, we have a widest interval where $z = 40$ and $z = 60$ with the length of intervals, $1-\alpha$. It means our uncertainty on estimation keep increases from $z=10$ to $z=40$ and we have the highest uncertainty near the separation occurs. Then it diminishes as it furthers away from the boundary of the separation. In $\texttt{glmdr}$, $\texttt{inference}$ function provides this confidence intervals using the sequential quadratic programming (SQP) to solve the constrained nonlinear problem \eqref{CI_logistic}. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{Figures/One_sided_CI.png}
    \caption{One-sided 95\% confidence interval for the example of complete separation from Section \ref{sec:complete_separation}. Solid dot represents the observed value and bar shows the interval. $\hat{p}_x$ is the estimated probability of a success given $z$. }\label{Fig:One_sided_CI}
\end{figure}

\subsection{Prediction}

Model based prediction is different when data separation is present. One difference is that standard techniques fail in the presence of data separation. In the absence of separation we can compute the predicted value for new data point from the logistic model using $\hat{p}_{x_{\text{pred}}} = (1 + \exp{(-x_{\text{new}}^T \hat{\beta})})^{-1}.$ However, when complete separation presents, this approach fails since $\hat{\beta}$ is at infinity. Standard estimates of variability suffer from a similar problem. Another difficulty is due to uncertainty in the separation itself. Data separation occurs with probability tending to zero as the sample size increases with the number of predictors fixed. Data separation is a sampling issue, not a modeling issue. We propose a new two-stage method for prediction that addresses the practical and conceptual difficulties of prediction when separation exists. 
%that we fit two possible models for new data point with different value of a outcome variable then compute the weighted conditional probability of a success.

This method is as follows: first, pick a point $x_{\text{new}}$ for which a prediction is desired and separation exists at $x_{\text{new}}$, a prediction at $x_{\text{new}}$ is either 0 or 1 using traditional methods. We combine this point with the observed data. We will make a model-based prediction at $x_{\text{new}}$ by fitting separate logistic regression models, one outcome label $y_{\text{new}}=0$ and the other with $y_{\text{new}}=1$. Fitting two separate models in this way is intended to address the uncertainty in the data separation. We then compute the estimated probability of a success for new data points, $\hat{p}_{x_\text{new}0}$ and $\hat{p}_{x_\text{new}1}$. Note that one of $\hat{p}_{x_\text{new}0}$ or $\hat{p}_{x_\text{new}1}$ will be 0 or 1 and the other will not be, this is because $y_{\text{new}}=0$ or $y_{\text{new}}=1$ decreases the uncertainty in the separation by adding a pseudo outcome in its favor, while the other pseudo outcome alleviates the separation at $x_{\text{new}}$. We now combine $\hat{p}_{x_\text{new}0}$ and $\hat{p}_{x_\text{new}1}$ to form a prediction using model averaging. Our model averaging procedure judges the fit of each model based on weights similar to the the Akaike weights in \cite{burnham_anderson_2002}. These weights are 
$$
  w_j = \frac{{\exp(-\frac{IC_j}{2})}}{\exp(-\frac{IC_1}{2})+\exp(-\frac{IC_2}{2})},
$$ 
  where $IC_j$ is the information criteria of model $j$. Then we can calculate the model averaged estimate, $\hat{p}^{*}_{x_{\text{new}}} = \sum_{j=0}^{1} w_j \hat{p}_{x_\text{new}j}$. We used Akaike information criteria corrected (AICc) as $IC_j$. The primary reason for its use is that AICc does not have an overfit problem when the sample size is small \citep{AICc_Paper}. The presence of data separation is an indication that one is not close to asymptopia. We then label $1$ if $\hat{p}^{*}_{x_{\text{new}}} \geq C^*$ and $0$ if $\hat{p}^{*}_{x_{\text{new}}} < C^*$ where $C^*$ is the optimal cut-off that maximizes the overall accuracy. The main motivation of using optimal cut-off is that threshold of $0.5$ produces unreliable and poor model accuracy when the outcome variable is highly unbalanced \citep{Freeman_2008}. For prediction intervals, we construct the Wilson intervals [\citeyear{wilson_1927}] for predicted probabilities. Wilson intervals show better coverage probability although $\hat{p}^{*}_{x_{\text{new}}}$ is near $0$ and $1$ boundaries in comparison to the standard binomial confidence interval because Wilson intervals are asymmetric \citep{Brown2001}. Detailed implementation and examples are given in the supplementary materials.

%Given new data $x_{\text{new}}$ and training set $x_{\text{train}}$, we generate testing set by combing training set and each observation from new data. That is, $x_{i,\text{test}} = x_{\text{train}} \cup x_{i,\text{new}}$ where $i$ is a index of whole new data. Then, we construct two testing labels that one has $y_{\text{new}}=0$ and the other has $y_{\text{new}}=1$ for new data point. Based on these two datasets, we fit two logistic models to compute the estimated probability of a success for new data points, $\hat{p}_{x_0}$ and $\hat{p}_{x_1}$. Since we do not know which model is fitted from the true value of outcome variable, we compare the weight of evidence for each model based on the Akaike weights for the model selection \citep{burnham_anderson_2002}. Let $w_j$ be the weight for model $j$ defined by:
%$$w_j = \frac{{\exp(-\frac{IC_j}{2})}}{\exp(-\frac{IC_1}{2})+\exp(-\frac{IC_2}{2})},$$ where $IC_j$ is the information criteria of model $j$. Then we can calculate the model averaged estimate, $\hat{p}^{*}_{x} = \sum_{j=0}^{1} w_j \hat{p}_{x_j}$. This averaged estimate is especially useful for prediction in our framework because we can use all predicted probabilities from models we have. For $IC$, since our sample size is more likely to be small when the complete separation presents, we recommend the Akaike information criteria corrected (AICc). The primary reason is that AICc does not have an overfit problem despite of small sample size \citep{AICc_Paper}. Also, it converges to Akaike information criteria (AIC) when we have large sample size, and AIC is asymptotically equivalent to choice of model by leave-one-out cross validation \citep{Stone_1977}. Meanwhile, Bayesian information criteria (BIC) attempts to find the true model among the sets of candidate models which is not appropriate our prediction framework \citep{schwarz_1978}. We then label $1$ if $\hat{p}^{*}_{x} \geq C^*$ and $0$ if $\hat{p}^{*}_{x} < C^*$ where $C^*$ is the optimal cut-off that maximizes the overall accuracy. The main motivation of using optimal cut-off is that threshold of $0.5$ produces unreliable and poor model accuracy when the outcome variable is highly unbalanced \citep{Freeman_2008}. For prediction intervals, we construct the Wilson intervals [\citeyear{wilson_1927}] for predicted probabilities. Wilson intervals show better coverage probability although $\hat{p}_{x}$ is near $0$ and $1$ boundaries in comparison to the standard binomial confidence interval because Wilson intervals are asymmetric \citep{Brown2001}. Detailed implementation and examples are given in the supplementary materials.

\subsection{Model Performance}
To compare $\texttt{glmdr}$ and other models ($\texttt{bayesglm}$, $\texttt{brglm}$ ($\texttt{logistf}$), and linear model), we measure the in-sample accuracy and confidence intervals for the inferential ability, and out-of-sample accuracy, prediction intervals, and computational cost for the predictive performance. In inference, we compute the in-sample accuracy as the number of correctly classified observations in the training set divided by total number of observations in the training set. For confidence intervals, we only consider observations that occur the (quasi) complete separation. Then, we compute the average length of one-sided confidence interval for $\texttt{glmdr}$ and average length of Wilson intervals for $\texttt{bayesglm}$,  $\texttt{brglm}$ ($\texttt{logistf}$) and linear models (since the predicted value of linear model does not have to fall into $[0,1]$ range, we assign $1$ for any predicted values greater than $1$ and $0$ for negative values).

In prediction, we use the leave-one-out cross validation (LOOCV) for out-of sample accuracy, which is total number of correctly classified observation in the testing set divided by total size of the testing set. We calculate the Wilson intervals for the prediction intervals, and $\texttt{proc.time}$ function in R to measure the execution time for the computational cost.

\subsection{Data}

We provide inference and prediction results for the maize data as well as an extensive set of didactic examples. These include: 

\noindent\textbf{Complete separation}: This example comes from  \cite{agresti_categorical_2013} and is discussed in Section~\ref{sec:complete_separation}. In this example, there is a binary outcome variable, $y \in \{0,1\}$ and one covariate variable, $z$, with 8 data points. Specifically, $y_i = 1$ at $z = 10, 20, 30, 40, $ and $y_i = 0$ at $z = 60, 70, 80, 90$. Since $y$ could be completely separable by $z$, we observed the complete separation in this example. \\

\noindent\textbf{Quasi-complete separation}: 
 This example is an extension of the complete separation example in \cite{agresti_categorical_2013} with two points added, $y_i=1$ and $y_i=0$ at $z = 50$. This is an example of quasi-complete separation. \\

\noindent\textbf{Quadratic logistic regression model}: This example comes from Section 2.2 of Geyer [\citeyear{Geyer2009}]. There is one binary outcome variable $y \in \{0,1\}$ and one covariate variable $z$ which takes integer values from 1 to 30. The outcome variable was $y_i =1$ when $ 12 < z_i < 24 $ and $y_i = 0$ otherwise. A quadratic logistic model is considered in this example and complete separation is observed. \\

\noindent\textbf{Endometrial Cancer Study}: This example comes from \cite{Heinze2002}. %\cite{Heinze2002} investigated the endometrial data set (n = 79), which was originally provided by Dr. Asseryanis from the Vienna University Medical School. 
The main purpose of this study was to describe histology of cases (HG) in terms of three risk factors: neovasculation (NV), endometrium height (EH) and pulsatility index of arteria uterina (PI). The outcome variable had 30 patients classified grading 0-II for histology (HG = 1) and 49 patients for grading III-IV (HG = 0). There were 13 patients who had neovasculization (NV = 1) and absent for 66 patients (NV = 0). Pulsatility index (PI) ranges from 0 to 49 with mean of 17.38 and median of 16.00, and endometirum height (EH) ranges from 0.27 to 3.61 with mean of 1.662 and median of 1.640. Quasi-complete separation was observed in this example, this separation is determined by NV. \\ 

\noindent\textbf{Maize data}: This example comes from \cite{Romay2013}, and it consists of 2,815 maize lines. The binary outcome variable is the kernel color, where 1 indicated non-white kernel color and 0 indicated white kernel color. We fitted a logistic regression model with kernel color as the outcome variable and covariate variables consisting of subpopulations and 24 DNA markers surrounding the \textit{psy1} gene. Each marker had a value from 0 to 1. In the final data set, 309 lines had a white kernel and 1,238 had non-white kernel color. These maize lines were subdivided into six subpopulations, namely 115 non-stiff stalk, 54 popcorn, 120 stiff stalk, 116 sweet corn, 159 tropical, and 983 unclassified. In this example, there was no separation issues when we used a single marker for covariate. However, we had a separation issue for saturated model with 24 DNA markers and subpopulations.

\subsection{Materials}

We implemented our methodology in R package $\texttt{glmdr}$. We used R version 3.6.1 and the required R packages for $\texttt{glmdr}$ is $\texttt{nloptr}$ version 1.2.2.2. To compare its performance, we considered $\texttt{arm}$ version 1.11-1, $\texttt{brglm2}$ 0.7.0, $\texttt{logistf}$ version 1.23 and $\texttt{stats}$ version 3.6.1. To determine the optimal cut-off for the logistic regression, we used $\texttt{PresenceAbsence}$ version 1.1.9. For visualization, data wrangling and experiments, we used $\texttt{ggplot2}$ version 3.3.3, $\texttt{gridExtra}$ version 2.3, $\texttt{latex2exp}$ version 0.4.0, $\texttt{foreach}$ version 1.4.7, $\texttt{doParallel}$ version 1.0.15, and $\texttt{tidyverse}$  version 1.2.1. Further details are included in the technical reports. $\texttt{glmdr}$ is available on \url{https://github.com/DEck13/complete_separation}.

\section{Results}

\subsection{Inference}
\label{sec:inference}

We report the in-sample accuracy for all observations and confidence intervals for problematic observations that raise the (quasi) complete separation issue to compare each method. For $\texttt{brglm}$, it is theoretically equivalent to the $\texttt{logistf}$ when $\texttt{brglm}$ uses the maximum penalized likelihood with powers of the Jeffreys prior as penalty. However, $\texttt{brglm}$ fails to converge for the maize example, meanwhile, $\texttt{logistf}$ converges. Therefore, we use $\texttt{logistf}$'s result for $\texttt{brglm}$ in maize example. For confidence intervals, we compute the average length of one-sided confidence interval for $\texttt{glmdr}$ and average length of Wilson intervals for $\texttt{bayesglm}$,  $\texttt{brglm}$ ($\texttt{logistf}$) and linear models. In Table \ref{Tab:inference}, we can see all methods show the equivalent in-sample accuracy for the complete separation and quasi separation examples. Meanwhile, the logistic models, $\texttt{glmdr}$, $\texttt{bayesglm}$, and  $\texttt{brglm}$ ($\texttt{logistf}$), display the higher in-sample accuracy for quadratic, endometrial, and maize examples in comparison to the linear model. Within these examples, $\texttt{glmdr}$ has the highest in-sample accuracy in maize example than other two logistic models. For confidence intervals, $\texttt{glmdr}$ demonstrates the smallest length in all examples. Especially, in quadratic and endometrial examples, its lengths of confidence intervals are significantly smaller than other methods. Two logistic models, $\texttt{bayesglm}$ and $\texttt{brglm}$ ($\texttt{logistf}$) generally shows smaller lengths of confidence intervals but they are not highly different from that of linear model in all examples. This result suggests that linear model perform worse than logistic models, and $\texttt{glmdr}$ which solves the complete separation within the MLE framework produces the most accurate inference for (quasi) complete separation problem.

\begin{table}[!ht]
\caption{Model performances for all examples.}
\caption*{\footnotesize \textit{glmdr denotes Generalized Linear Model Done Right \citep{glmdr_package}, bayesglm denotes Generalized Linear Model with Student-t prior distribution \citep{Gelman2008}, brglm denotes Bias Reduction in Generalized Linear Models \citep{Kosmidis2009}, logistf denotes Logistic model with Firth's modified score function \citep{Heinze2002}, and linear denotes the multiple linear model using ordinary least squares.}}\label{Tab:inference}
\centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|ccccc}
 &  & Complete Separation & Quasi Separation & Quadratic & Endometrial & Maize \\ \hline
accuracy & glmdr & 100 \% & 90 \% & 100 \% & 88.61 \% & 87.14 \% \\
 & bayesglm & 100 \% & 90 \% & 100 \% & 88.61 \% & 87.07 \% \\
 & brglm / logistf & 100 \% & 90 \% & 100 \% & 88.61 \% & 87.01 \% \\
 & linear & 100 \% & 90 \% & 90 \% & 86.08 \% & 86.81 \% \\
length & glmdr & 0.550 & 0.308 & 0.199 & 0.194 & 0.563 \\
 & bayesglm & 0.828 & 0.827 & 0.823 & 0.804 & 0.814 \\
 & brglm / logistf & 0.835 & 0.831 & 0.811 & 0.808 & 0.826 \\
 & linear & 0.829 & 0.829 & 0.859 & 0.806 & 0.838
\end{tabular}
    }
\end{table}

\subsection{Prediction}
\label{sec:prediction}

To compare the performance of prediction, we compare out-of-sample accuracy, prediction intervals and computational cost. In Table \ref{Tab:prediction}, we can see all methods show the same accuracy for the complete separation and quasi separation examples. $\texttt{glmdr}$ shows the highest out-of-sample accuracy in endometrial example where other three methods perform the same. In quadratic example, $\texttt{brglm}$ performs the best followed by other two logistic models and linear model, but linear model is better than the logistic models in maize example although their differences are not large. This result is surprising because the linear model is generally not recommended for binary classification, yet it shows a better performance than the logistic models. For prediction intervals, overall there is no significant difference between each method. We notice that $\texttt{glmdr}$ has the smallest lengths of prediction intervals in all examples but for the quasi complete separation example where the linear model displays the smallest length of prediction intervals.

\begin{table}[!hb]
\caption{Prediction results and computational cost for all examples.}
\caption*{\footnotesize \textit{glmdr denotes Generalized Linear Model Done Right \citep{glmdr_package}, bayesglm denotes Generalized Linear Model with Student-t prior distribution \citep{Gelman2008}, brglm denotes Bias Reduction in Generalized Linear Models \citep{Kosmidis2009}, logistf denotes Logistic model with Firth's modified score function \citep{Heinze2002}, and linear denotes the multiple linear model using ordinary least squares.}}\label{Tab:prediction}
\centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|ccccc}
 &  & Complete Separation & Quasi Separation & Quadratic & Endometrial & Maize \\ \hline
accuracy & glmdr & 100 \% & 80 \% & 93.33 \% & 87.34 \% & 86.04 \% \\
 & bayesglm & 100 \% & 80 \% & 93.33 \% & 86.08 \% & 86.36 \% \\
 & brglm / logistf & 100 \% & 80 \% & 100 \% & 86.08 \% & 86.30 \% \\
 & linear & 100 \% & 80 \% & 90 \% & 86.08 \% & 86.55 \% \\
length & glmdr & 0.822 & 0.859 & 0.807 & 0.839 & 0.836 \\
 & bayesglm & 0.839 & 0.845 & 0.828 & 0.843 & 0.837 \\
 & brglm / logistf & 0.843 & 0.847 & 0.813 & 0.844 & 0.837 \\
 & linear & 0.833 & 0.844 & 0.861 & 0.851 & 0.839 \\
cost & glmdr & 0.13 secs & 0.27 secs & 0.31 secs & 1.06 secs & 4.74 mins \\
 & bayesglm & 0.11 secs & 0.12 secs & 0.35 secs & 0.31 secs & 45.35 secs \\
 & brglm / logistf & 0.19 secs & 0.19 secs & 0.44 secs & 0.49 secs & 2.26 hours \\
 & linear & 0.07 secs & 0.06 secs & 0.09 secs & 0.14 secs & 4.63 secs
\end{tabular}}
\end{table}

We present the computational cost of each method in Table \ref{Tab:prediction}. In all examples, linear model is much faster than logistic models. Although there is no significant difference in complete separation, quasi complete separation, quadratic, and endometrial examples, computational cost of $\texttt{glmdr}$ increases much in maize example because execution time for $\texttt{glmdr}$ increases as it requires more computations to solve the optimization problem if the data point to be predicted occur the separation. Similarly, $\texttt{brglm}$ is notably slow because it needs to handle the optimization problem to find the penalized MLE for each iteration. However, $\texttt{bayesglm}$ does not suffer this issue because it does not carry the computation for the optimization problem in their method.

Considering all aspects, all of four methods demonstrate comparable out-of-sample accuracy and length of prediction intervals. However, there are several notable differences. $\texttt{glmdr}$ provides the smallest lengths of prediction intervals except in the quasi separation example. It also shows better performance in endometrial example. But, it may not be scalable to the large datasets due to relatively high computational cost. $\texttt{bayesglm}$ performs well on all examples with the lowest computational cost, which indicates the $\texttt{bayesglm}$ is suitable for prediction on large data. $\texttt{brglm}$ achieves the highest out-of-sample accuracy in the quadratic example, but $\texttt{brglm}$ fails to converge in maize example and alternative method, $\texttt{logistf}$, is very costly. Meanwhile, the linear model performs well despite of the binary outcome. It shows comparable or better out-of-sample accuracy with small prediction intervals and the lowest computational cost.

\section{Discussion}

In the classification problem, the logistic model is one of the most common statistical model we can attempt. Although linear model is attractive option to use because of its easiness and handiness, the binary outcome variable makes the linear model violate necessary assumptions such as homoscedasticity and linearity (i.e. Gauss-Markov assumptions) as well as normality. Therefore, even though results from Section \ref{sec:inference} and \ref{sec:prediction} display that the performance of linear model is comparable to the logistic models, we can not fully utilize asymptotic properties of linear model and make a proper inference such as significance tests for coefficients.

On the other hand, $\texttt{glmdr}$ is considered to be the most preferable logistic model based on its overall performance in the inference and prediction. The main strength of $\texttt{glmdr}$ is it provides the best inference as the way that $\texttt{glmdr}$ handles the separation problem is the true remedy to the traditional $\texttt{glm}$'s separation issue. It solves the separation issue within the maximum likelihood estimation framework unlike other two logistic models and estimates the probability of success by finding the MLE in the Barndorff-Nielsen completion [\citeyear{barndorff-nielsen_1978}] based on approximate null eigenvectors of the Fisher information matrix. Meanwhile, other two logistic models solve the separation problem by switching the problem settings. For example, $\texttt{bayesglm}$ adopts a Bayesian approach which scales the data first and then places Cauchy distribution as a prior distribution on the coefficients and $\texttt{brglm}$ modifies the score function to produce finite coefficients. As a result, not only are both models' results in inference not the best, but it is also hard to see their outputs as a true solution for separation problem of $\texttt{glm}$. In prediction, $\texttt{glmdr}$ shows similar or better out-of-sample accuracy when the quasi-complete separation presents, and comparable performance when the complete separation exists with the narrowest length of prediction intervals with acceptable computational cost. It may take much time when we have a large number of observations, but the complete separation is likely to occur when we have a small sample size. Thus, high computational cost in large sample size should not be the major issue in $\texttt{glmdr}$.

In conclusion, when separation issue present in the logistic model, one can consider using the $\texttt{glmdr}$ which has the advantage in inference and the comparable prediction power. $\texttt{bayesglm}$ is suitable for prediction in large datasets thanks to its low computational cost yet high accuracy. $\texttt{brglm}$ or $\texttt{logistf}$ may be least preferable method because they are computationally unstable and expensive.

\bibliographystyle{abbrvnat}
\bibliography{Reference}


\end{document}