\documentclass[11pt]{extarticle}

\usepackage{adjustbox}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{dirtytalk}
\usepackage{xcolor}
 \usepackage{multirow}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[square]{natbib}
\makeatletter
\newcommand{\bianca}{\renewcommand\NAT@open{[}\renewcommand\NAT@close{]}}
\makeatother
\newcommand*\sqcitep[1]{{\bianca\citep{#1}}}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{threeparttable}
\usepackage{booktabs, caption, makecell}
\usepackage{graphics}
\usepackage{threeparttable}

\newtheorem{thm}{Theorem}
\newcommand{\Gamlim}{$\Gamma_{\text{lim} \text{ }}$}
\newenvironment{commentline}[1]{
\noindent{\color{red} \rule{\linewidth}{0.5mm}}
\textbf{Comment}: #1

\noindent{\color{red} \rule{\linewidth}{0.5mm}}
}

\title{\textbf{Robust model based prediction of gene expression in maize}}
\author{Suyoung Park, Alex E. Lipka, Daniel J. Eck\\
\emph{University of Illinois at Urbana-Champaign}}
\date{Month 2021}

\begin{document}
\maketitle
\begin{abstract}
  Help us with the title Alex, you're our only hope!
\end{abstract}
\smallskip
\noindent \textbf{Key Words:} list of keywords


\section{Materials and Method}

\subsection{Material}
We implemented our methodology in R package $\texttt{glmdr}$. We used R version 3.6.1 and the required R packages for $\texttt{glmdr}$ is $\texttt{nloptr}$ version 1.2.2.2. Further details are included in the technical reports.

\subsection{Data}

We provide inference and prediction results for the maize data as well as an extensive set of examples. These include: 

\noindent\textbf{Complete separation}: We first analyze the \cite{agresti_categorical_2013} example discussed in Section~\ref{sec:complete_separation}. \\ 

\noindent\textbf{Quasi-complete separation}: 
 We analyze the \cite{agresti_categorical_2013} example with two points added, a success and a failure at $x = 50$. \\


\noindent\textbf{Quadratic logistic regression model}: This example comes from Section 2.2 of Geyer [\citeyear{Geyer2009}]. Let $y_i =1$ for $ 12 < x_i < 24 $ and $y_i = 0$, otherwise. Also, consider the following quadratic model: 
$$\text{logit}(p_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2.$$
In this case, maximum likelihood estimate (MLE) does not exist when we fit the logistic model using $\texttt{glm}$ and it complains that the algorithm did not converge. We demonstrate how to compute the one-sided confidence intervals for mean value parameters for this example in the supplementary material. \\

\noindent\textbf{Endometrial Cancer Study}: \cite{Heinze2002} firstly investigated the endometrial data set (n = 79), which was originally provided by Dr. Asseryanis from the Vienna University Medical School. The main purpose of this study was to describe histology of cases (HG) in terms of three risk factors: neovasculation (NV), endometrium height (EH) and pulsatility index of arteria uterina (PI). 30 patients was classified grading 0-II for histology (HG = 1) and 49 patients for grading III-IV (HG = 0). There are 13 patients who has neovasculization (NV = 1) and absent for 66 patients (NV = 0). Pulsatility index (PI) ranges from 0 to 49 with mean of 17.38 and median of 16.00, and endometirum height (EH) ranges from 0.27 to 3.61 with mean of 1.662 and median of 1.640. In this example, we observe the quasi-complete separation in NV.  \\

\noindent\textbf{Maize data}: To predict the kernel color of maize, we merged two datasets on accession's name. One dataset comes from \cite{Romay2013}'s work that investigates the genetic constitution of 2,815 maize inbred accessions with 7 types of population structures. [Place for description of the kernel color dataset] The other dataset contains the kernel color of accession where 1 indicates yellow kernel and 0 for white kernel. It has 24 marker genotypes for the DNA surrounding a biologically relevant gene for kernel color. Each marker has value from 0 to 1. In the final dataset, 309 observations have a white kernel and 1,238 for yellow kernel. We have 6 types of population structures: 115 non-stiff stalk, 54 popcorn, 120 stiff stalk, 116 sweet corn, 159 tropical, and 983 unclassified. In this example, there is no separation issues when we use single marker for explanatory variable. However, we have a separation issue for saturated model. In the later part, we mainly focus on this example.

\subsection{Logistic Regression}
The logistic regression is the special case of the generalized linear model which the response variable follows Bernoulli distribution (i.e., $y \in \{0,1\}$) \sqcitep{GLM_paper}. By convention, we encode 1 as a ``success" and 0 as a ``failure." In logistic regression the conditional success probability at a particular $x$ is modeled as
\begin{equation}\label{logistic_model}
\text{Pr}(Y_i = 1 | X_i=x_i) = p_i = \frac{\exp{(x_i^{T}\beta)}}{1+\exp{(x_i^{T}\beta)}},
\end{equation}
where $\beta$ is an unknown parameter vector.

From the linear regression's point of view, this logistic regression is equivalent to:
\begin{equation}\label{link_function}
g(p_i) = \log(\frac{p_i}{1-p_i}) = x_i^T\beta
\end{equation}
where $g(x) = \log(\frac{x}{1-x})$ is a logit link (log-odds ratio).

Therefore, as in classical ordinary least squares (OLS) regression, we can estimate $\beta$ using maximum likelihood estimation and make statistical inferences about regression coefficient estimates via the diagonal elements of the inverse of the Fisher information, which represent the estimated variance of parameters.


Unlike in OLS regression, the parameter estimates $\hat\beta$ are not given in closed form. The log-likelihood function for the logistic regression model is
\begin{equation}\label{likelihood}
\log L(\beta|Y) = \sum_{i=1}^{n} y_i\log{(p_i)} + (1-y_i)\log{(1-p_i)},
\end{equation}
one then obtains $\hat\beta$ by solving the score function equation
\begin{equation}\label{logit_Score}
\frac{\partial \log{L(\beta|Y)}}{\partial \beta} = 
\sum_{i=1}^{N} (y_i - \log{(p_i)}) x_i^T =
\sum_{i=1}^{N} [y_i + \log{(1+\exp{(-x_i^T\beta)})}] = 0.
\end{equation}
Conventional softwares finds $\hat\beta$ through Fisher-scoring or iteratively reweighted least squares algorithms \citep[Chapter 4]{agresti_categorical_2013}. We then obtain inferences using an estimate of the Fisher information matrix evaluated at the MLE solution $\hat{\beta}$
\begin{equation}\label{logit_Fisher}
\widehat{\text{Var}(\beta)} = [I(\hat\beta)]^{-1} = \left(-E\left[\frac{\partial^2 \log{L(\beta|Y)}}{\partial \beta_i \partial \beta_j}\right]\right)^{-1}\Big\vert_{\beta = \hat\beta}.
\end{equation}
Conventional software provides \eqref{logit_Fisher}.


\subsection{Mean-value Parameters}
From the statistical model, what we actually want to know is the expected value of response variable given data. In the linear model, we can easily obtain this expected value because $\text{E}(Y|X=x) = x^{T}\beta$ and by plugging in $\hat{\beta}$, we can get $\hat{y}.$ On the other hands, in the logistic model, $\text{E}(Y|X=x) = \text{Pr}(Y = 1 | X=x) = p_i$ and $\log(\frac{p_i}{1-p_i}) = x_i^T\beta.$ Therefore, we cannot obtain the expected value of response variable in the same way as we did in the linear model.

To get the expected value from the logistic model, we can consider mean-value parameterization. Instead of directly using the estimated coefficients of the logistic regression model, we can have the estimated conditional probability of success given data by plugging in $\hat{\beta}$ into \eqref{logistic_model}. The advantage of this parameterization is now our parameters of interest shifts to the mean-value parameters from the coefficients of the model. Consequently, we can provide a more informative and intuitive inference. For example, we can tell the expected probability of success at particular $x$ which is what we desire from the statistical model (without mean-value parameterization, our interpretation on model is that as one unit of explanatory variable increases the expected change in log odds ratio of conditional probability of success is $\hat{\beta}$).


\subsection{Complete Separation}
\label{sec:complete_separation}

The traditional maximum likelihood estimation does not work well when there is complete or quasi-complete separation in the data. \cite{agresti_categorical_2013} defines complete separation when there exists a vector $b$ such that

\begin{equation}\label{definition_complete_separation}
\begin{split}
x_{i}^{T} b &> 0 \text{ whenever } y_i = 1, \\
x_{i}^{T} b &< 0 \text{ whenever } y_i = 0.
\end{split}
\end{equation}
That is, it occurs when the one or more explanatory variables can perfectly predict the response variable \sqcitep{AlbertAnderson1984}. For example, as shown in the Figure~\ref{Fig:Complete_Separation_Example}, consider the following case that when $x$ is less than 50, all corresponding $y$ are $0$ and when $x$ is greater than 50, all corresponding $y$ are $1$. Suppose we are interested in a simple logistic regression model $x^T = [1, x_i]$. Then this data is completely separated with $b = [-50, 1]^T.$ Moreover, we have $\hat{p} = 0$ for $x < 50$ and $\hat{p} = 1$ for $x > 50$.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{Figures/Agresti_Complete_Separation.png}
    \caption{Example of complete separation from Section 6.5.1 of \cite{agresti_categorical_2013}. The conventional MLE of a logistic model does not exist.}\label{Fig:Complete_Separation_Example}
\end{figure}

When there is complete separation, the parameter estimates $\hat\beta$ as at infinity, the iteration based estimation algorithms provide a sequence of estimates that goes to infinity, and the log likelihood becomes flat when evaluated along this sequence. The left panel of Figure \ref{Fig:asymptotes} shows the log likelihood of logistic model for this example with different working estimate from $\texttt{glm}$ function in R. We can see that each iteration, norm of $\beta$ becomes larger and asymptote of the log likelihood value goes to infinity. The right panel of Figure \ref{Fig:asymptotes} is the zoomed part of the left panel of Figure \ref{Fig:asymptotes} where the log of norm of working estimates is between 4.5 and 5. It displays the log likelihood value still approaches near zero although the left panel of Figure \ref{Fig:asymptotes} looks flat in the same region. In complete separation, the usual statistical inference is not valid. %The variance of $\hat{\beta_j}$ becomes very large because it comes from the inverse of second derivatives of the likelihood function \eqref{logit_Fisher}, and%
The standard errors of predicted probabilities of success are very small, which leads to extremely narrow confidence intervals for each observation. Unfortunately, none of common statistical software such as R, SAS and Python can handle the separation issue properly and uninformed users sometimes uses the wrong model without knowing it. The \texttt{glmdr} software package is designed to provide users with a description of the complete separation problem when it occurs, and provide statistical inferences when it occurs.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{Figures/asymptotes.png}
    \caption{\textbf{Left panel}: Log likelihood values of logistic model at different working estimates. Blue dot represents the log likelihood value at each iteration. \textbf{Right panel:} Zoom in view of a log likelihood values of logistic model where log of norm of working estimates lie between 4.5 and 5.}\label{Fig:asymptotes}
\end{figure}

Quasi-complete separation is another case of separation that there are both a success and a failure on the hyperplane that separates the successes from the failures \sqcitep{lesaffre_albert_1989}. For instance, we can consider additional two points that $x = 50$ with $y=1$ and $y=0$ to the previous complete separation example. That is, we have $y_i = 0$ for $x \leq 50$ and $y_i =1$ for $x \geq 50$. In this case, the maximized log likelihood is always negative and we experience same phenomenon as the complete separation case.

\subsection{One-Sided Confidence Interval}
We use one-sided confidence intervals for the logistic model's mean value parameters to explain the uncertainty of estimation. Original concept can be found in Section 3.16 of Geyer's paper [\citeyear{Geyer2009}] and implementation details can be found in Section 4.3 of \citeauthor{Eck2021}'s work [\citeyear{Eck2021}]. Briefly, we construct confidence interval for mean value parameters such that one endpoint is observed response variable (i.e., lower bound if $y_i=0$ and upper bound if $y_i=1$) and the other endpoint is obtained by solving the optimization problem:

\begin{equation} \label{CI_logistic}
\begin{split}
    \text{minimize } & \quad -\theta_k \\
    \text{subject to } & \sum_{i \in I}[y_{i} \log{(p_i)} + (1 - y_i) \log{(1-p_{i}})] - \log{(\alpha) \geq 0},
\end{split}
\end{equation}
where $\theta_k = x_k^T\beta$ for any $k \in I$, $\textit{I}$ is a index of problematic points that cause the separation, $p$ is a mean value parameter, and $\alpha$ is a significance level. For example, Figure \ref{Fig:One_sided_CI} shows the one-sided confidence interval for the complete separation example we discussed in Section \ref{sec:complete_separation}. We can see the confidence interval increases as $x$ increases until $x = 40$ then it starts to decrease as $x$ increases from $x = 60$. Also, we have a widest interval where $x = 40$ and $x = 60$ with the length of intervals, $1-\alpha$. It means our uncertainty on estimation keep increases from $x=10$ to $x=40$ and we have the highest uncertainty near the separation occurs. Then it diminishes as it furthers away from the boundary of the separation. In $\texttt{glmdr}$, $\texttt{inference}$ function provides this confidence intervals using the sequential quadratic programming (SQP) to solve the constrained nonlinear problem \eqref{CI_logistic}. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{Figures/One_sided_CI.png}
    \caption{One-sided 95\% confidence interval for the example of complete separation from Section \ref{sec:complete_separation}. Solid dot represents the observed value and bar shows the interval. $\hat{p}$ is the estimated probability of a success given $x$. }\label{Fig:One_sided_CI}
\end{figure}

\subsection{Prediction}

Prediction in $\texttt{glmdr}$ framework is different from that of the conventional statistical model because we do not have a finite estimate. Specifically, in the traditional sense, we can compute the predicted value for new data point from the logistic model using $\hat{p}_{pred} = (1 + \exp{(-x_{new}^T \hat{\beta})})^{-1}.$ However, when the complete separation presents, this approach does not work. Therefore, we propose a new method for the prediction that we fit two possible models for new data point with different value of a response variable then compute the weighted conditional probability of a success.

Given new data $x_{\text{new}}$ and training set $x_{\text{train}}$, we generate testing set by combing training set and each observation from new data. That is, $x_{i,\text{test}} = x_{\text{train}} \cup x_{i,\text{new}}$
where $i$ is a index of whole new data. Then, we construct two testing labels that one has $y_{\text{new}}=0$ and the other has $y_{\text{new}}=1$ for new data point. Based on these two datasets, we fit two logistic models to compute the estimated probability of a success for new data points, $\hat{p_1}$ and $\hat{p_2}$. Since we do not know which model is fitted from the true value of response variable, we compare the weight of evidence for each model based on the Akaike weights for the model selection \sqcitep{burnham_anderson_2002}. Let $w_j$ be the weight for model $j$ defined by:
$$w_j = \frac{{\exp(-\frac{IC_j}{2})}}{\exp(-\frac{IC_1}{2})+\exp(-\frac{IC_2}{2})},$$ where $IC_j$ is the information criteria of model $j$. Then we can calculate the model averaged estimate, $\widehat{p^*} = \sum_{j=1}^2 w_j \hat{p_j}$. This averaged estimate is especially useful for prediction in our framework because we can use all predicted probabilities from models we have. For $IC$, we recommend the Akaike information criteria corrected (AICc) because Akaike information criteria (AIC) is asymptotically equivalent to choice of model by leave-one-out cross validation \sqcitep{Stone_1977}. Also, Bayesian information criteria (BIC) attempts to find the true model among the sets of candidate models which is not appropriate our prediction framework \sqcitep{schwarz_1978}. Furthermore, AICc works well despite of small sample size and converges to AIC when we have large sample size \sqcitep{AICc_Paper}. We construct the prediction intervals based on Wilson intervals given model averaged predicted value. Wilson intervals [\citeyear{wilson_1927}] are asymmetric unlike the  standard binomial confidence interval. Thus, Wilson intervals show better coverage probability although $\hat{p}$ is near $0$ and $1$ boundaries \sqcitep{Brown2001}. Lastly, we label the mean of this Wilson intervals. In our method, we label $1$ if $\widehat{p^*} \geq C^*$ and $0$ if $\widehat{p^*} < C^*$ where $C^*$ is the optimal cut-off that maximizes the overall accuracy. Detailed implementation and examples are given in the supplementary materials.

\subsection{Optimal Cut-off}

In the logistic regression, the most common threshold for mapping predicted probabilities of a success to a $0$ or $1$ label is $0.5$. However, this threshold is not often an optimal cut-off. \cite{Freeman_2008} find that the model accuracy is heavily affected by the choice of this threshold. Especially, they point out that threshold of $0.5$ produces unreliable and poor model accuracy when the response variable is highly unbalanced. In the literature, many criteria are suggested to optimize the threshold with regards to different metrics such as sensitivity, specificity and so on \sqcitep{Congalton_1991,fielding_bell_1997,Cantor_1999,Manel_2001}. Therefore, one can choose the most appropriate criteria for their goal. In our method, we select the threshold criteria such that maximizes the overall accuracy, called maximize percent correctly classified (MaxPCC). For the comparison of criteria and their implementation, see \cite{Freeman_2008} and the references therein.

\section{Results}

\subsection{Estimation}
\label{sec:estimation}

In maize data, we fit the logistic model where the response variable is kernel color. We dropped 5 markers out of 24 markers due to the collinearity issue and thus we use the population structures and 19 markers for the explanatory variables. We compare the in-sample accuracy and total length of confidence intervals among $\texttt{glmdr}$, $\texttt{bayesglm}$ \sqcitep{Gelman2008}, $\texttt{logistf}$ \sqcitep{Heinze2002} (we tested $\texttt{brglm2}$ \sqcitep{Kosmidis2009} but algorithm did not converge. Instead, we used $\texttt{logistf}$, which was equivalent to $\texttt{brglm2}$ with type of score adjustment as a maximum penalized likelihood with powers of the Jeffreys prior as a penalty) and multiple linear model. We report the average length of one-sided confidence interval for $\texttt{glmdr}$ and average length of Wilson intervals for each predicted probability from $\texttt{bayesglm}$,  $\texttt{logistf}$ and linear models for fair comparison (since the predicted value of linear model does not have to fall into $[0,1]$ range, we assign $1$ for any predicted values greater than $1$ and $0$ for negative values). In Table \ref{Tab:estimation}, we can see all model performs comparably but $\texttt{glmdr}$ provides narrowest length of confidence intervals which indicate the inference result of $\texttt{glmdr}$ is the most certain.

\begin{table}[!ht]
\caption{Model performances for all examples.}\label{Tab:estimation}
\centering
    \resizebox{\columnwidth}{!}{
    \begin{threeparttable}
    \begin{tabular}{lcccccccc}
    & \multicolumn{4}{c}{in-sample accuracy} &\multicolumn{4}{c}{average length of confidence intervals}\\ 
    & \multicolumn{1}{l}{\textbf{glmdr}\tnote{1} \textbf{ (Ours)}} & \multicolumn{1}{l}{bayesglm\tnote{2}} & \multicolumn{1}{l}{logistf\tnote{3} / brglm2\tnote{4}} & \multicolumn{1}{l}{linear\tnote{5}} & \multicolumn{1}{l}{\textbf{glmdr (Ours)} } & \multicolumn{1}{l}{bayesglm} & \multicolumn{1}{l}{logistf / brglm2} & \multicolumn{1}{l}{linear}  \\
    \hline
    Complete Separation & 100 \% & 100 \% & 100 \% & 100 \% & 0.55 & 0.83 & 0.84 & 0.83 \\
    Quasi Separation & 90 \% & 90 \% & 90 \% & 90 \% & 0.42 & 0.84 & 0.84 & 0.84 \\
    Quadratic & 100 \% & 100 \% & 100 \% & 90 \% & 0.20 & 0.82 & 0.81 & 0.86 \\
    Endometrial & 88.61 \% & 88.61 \% & 88.61 \% & 86.08 \% & 0.74 & 0.84 & 0.84 & 0.86 \\
    Maize & 87.14 \% & 87.07 \% & 87.01 \% & 86.81 \% & 0.82 & 0.84 & 0.84 & 0.84                      
    \end{tabular}
    \end{threeparttable}
    }
\end{table}


\footnotetext[1]{Our model, Generalized Linear Model Done Right from $\texttt{glmdr}$ package.}
\footnotetext[2]{Generalized Linear Model with Student-t prior distribution from $\texttt{arm}$ package \sqcitep{Gelman2008}.}
\footnotetext[3]{Logistic model with Firth's modified score function from $\texttt{logistf}$ package \sqcitep{Heinze2002}.} 
\footnotetext[4]{Bias Reduction in Generalized Linear Models from $\texttt{brglm2}$ package \sqcitep{Kosmidis2009}.}
\footnotetext[5]{Multiple Linear Model using ordinary least squares.}

\subsection{Prediction}
\label{sec:prediction}

We use the leave-one-out cross validation (LOOCV) for prediction. This setting is the most suitable as we want to predict the kernel color of new maize inbreed given our data. Table \ref{Tab:prediction} displays the out-of-sample accuracy, execution time and optimal cut-off from a MaxPCC for each data and method. Out-of-sample accuracy is calculated by sum of number of true positive and true negative divided by total number of samples in testing set and the execution time is measured by difference between the starting and ending time of the computation using $\texttt{proc.time}$ function in R. For accuracy, we can see all methods perform similarly in complete separation, quasi-complete separation and maize data. Also, logistic models perform better than linear model in quadratic and endometrial data. Meanwhile, for execution time, $\texttt{linear}$ model performs fastest followed by $\texttt{bayesglm}$, $\texttt{glmdr}$, and $\texttt{logistf}$/$\texttt{brglm2}$ is significantly slow.

\begin{table}[!ht]
\caption{Prediction results for all examples.}\label{Tab:prediction}
\centering
    \resizebox{\columnwidth}{!}{
    \begin{threeparttable}
    \begin{tabular}{lccccccccc}
    & \multicolumn{4}{c}{out-of-sample accuracy} &\multicolumn{4}{c}{execution time}&\multicolumn{1}{c}{cut-off}\\ 
    & \multicolumn{1}{l}{\textbf{glmdr}\tnote{1} \textbf{ (Ours)}} & \multicolumn{1}{l}{bayesglm\tnote{2}} & \multicolumn{1}{l}{logistf\tnote{3} / brglm2\tnote{4}} & \multicolumn{1}{l}{linear\tnote{5}} & \multicolumn{1}{l}{\textbf{glmdr (Ours)} } & \multicolumn{1}{l}{bayesglm} & \multicolumn{1}{l}{logistf / brglm2} & \multicolumn{1}{l}{linear}  \\
    \hline
    Complete Separation & 87.5 \% & 100 \% & 100 \% & 100 \%  & 0.13 secs & 0.11  secs & 0.186 secs & 0.07 secs & 0.465\\
    Quasi Separation & 90 \% & 80 \% & 80 \% & 80 \%  & 0.27 secs & 0.12 secs & 0.190 secs & 0.06 secs & 0.510\\
    Quadratic & 93.33 \% & 93.33 \% & 93.33 \% & 83.33 \% & 0.31 secs & 0.35 secs & 0.435 secs & 0.09 secs & 0.450\\
    Endometrial & 87.34 \% & 86.08 \% & 86.08 \% & 81.01 \% & 1.06 secs & 0.31 secs & 0.593 secs & 0.14 secs & 0.500\\
    Maize & 86.04 \% & 86.30 \% & 86.04 \% & 86.36 \% & 4.74 mins & 45.35 secs & 2.26 hours & 4.63 secs & 0.540                      
    \end{tabular}
    \begin{tablenotes}
      \small
      \item See the footnote on the bottom of page \pageref{Tab:estimation} for the meaning of each acronym.
    \end{tablenotes}
    \end{threeparttable}
    }
\end{table}

\section{Discussion}

In the classification problem, the logistic model is one of the most common statistical model we can attempt. Although linear model is attractive option to use because of its easiness and handiness, the binary response variable makes the linear model violate necessary assumptions such as homoscedasticity and linearity (i.e. Gauss-Markov assumptions) as well as normality. Therefore, even though results from Section \ref{sec:estimation} and \ref{sec:prediction} display that the performance of linear model is comparable to the logistic models, we can not fully utilize asymptotic properties of linear model and make a proper inference such as significance tests for coefficients.

On the other hands, we can see all logistic models in Section \ref{sec:estimation} and \ref{sec:prediction} perform similarly despite of different approaches and techniques. The main difference between $\texttt{glmdr}$ and other methods is that $\texttt{glmdr}$ is only model that solves the separation problem within the maximum likelihood estimation framework under the subset of the original model, called limiting conditional model (LCM). It estimates the probability of success by finding the MLE in the Barndorff-Nielsen completion [\citeyear{barndorff-nielsen_1978}] based on approximate null eigenvectors of the Fisher information matrix. Hence, the way $\texttt{glmdr}$ handles the separation problem is the true remedy to the traditional $\texttt{glm}$'s issue causing from a separation problem. Meanwhile, all other methods solve the separation problem by switching the problem settings. For example, $\texttt{bayesglm}$ uses a Bayesian approach which scales the data first and then placing Cauchy distribution as a prior distribution on the coefficients and $\texttt{logistf}$ (similar to $\texttt{brglm2}$) modifies the score function to produce finite coefficients. As a result, it is hard to see their outputs as a true solution for separation problem of $\texttt{glm}$.

In conclusion, when separation issue present in the logistic model, one can consider using the $\texttt{glmdr}$ which has the advantage in inference because it performs maximum likelihood estimation under the specified model. We see that this corresponds to the smallest confidence intervals in our examples, as expected. $\texttt{bayesglm}$ is suitable for prediction thanks to its low computational cost yet high accuracy. $\texttt{logistf}$ or $\texttt{brglm2}$ may be least preferable method because they are computationally unstable and expensive.



\bibliographystyle{abbrvnat}
\bibliography{Reference}


\end{document}