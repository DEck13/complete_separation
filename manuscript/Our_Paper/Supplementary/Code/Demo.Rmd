---
title: "Demo"
author: "Suyoung Park (spark148)"
date: "12/2/2020"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(readxl)
library(glmdr)
library(dplyr)
```

## Another apporach for finding LCM and linearity.

While I played with the data, I found subsetting unique values based on the decision boundary may produce the same results as glmdr.
Below is demo.

### Agresti's example (Completely Degenerated)

To my understanding, LCM model is the glm model with all non-problematic points (i.e. linearity is `TRUE`).
Thus, to find the LCM model, we need to find the problematic points and technically, `linearity`.
In `glmdr` function, we firstly find the MLE in completion using custom Newton method.
Then based on that, we approximate the null eigenvector of Fisher information to determine `linearity`.

```{r}
x <- c(10,20,30,40,60,70,80,90)
y <- c(rep(0,4),rep(1,4))
glmdr_model <- glmdr(y~x, family="binomial")
x[!glmdr_model$linearity]
```

As I discussed last meeting, I think that finding unique values of each distinct problematic predictor (i.e. variable that casues the separation) in terms of response variable are another way to find the problematic points (linearity == FALSE). For example, line 46 is the equivalent to the linearity==FALSE in logistic regression **when complete separation exists**.

* Comment: when quasi-separation presents or no separation exists, below code does NOT work but I think I can construct generic code using decision boundary which can be obtained by solving $logit^{-1}(\theta) \geq 0.5$ where $\theta = X\beta$ (line 53).

```{r}
dat <- data.frame(cbind(y,x))
cases <- data.frame(unique(x))
names(cases) <- "x"
cases[sapply(1:length(cases),function(i) nlevels(as.factor(
  inner_join(dat,cases[i,,drop=FALSE], by=c("x"))$y))==1),] #line 46
```

Visually, what I tried to do in line 46 was to see if there are both 0,1 in response variable for each space on decision boundary (i.e., check if there are 0 or 1 in left side of red-dashed line (decision boundary) and right side of red-dashed line).

```{r}
decision_boundary = -coef(glmdr_model$om)[1]/coef(glmdr_model$om)[2] # line 53
plot(y~x)
abline(v=decision_boundary,col="red",lty=2)
```

### Maize data

In the Maize data, I can obtain the problematic points using the same code (line 70th). In here, we do not need to find the decision boundary because dimension is already large enough each unique observation in our data plays a role of decision boundary (this is very "unmathematical" statement but I think I can construct formal way using series of decision boundaries). In line 70th, we can see they are the same. 

```{r}
dat <- read_xlsx("../manuscript/Supplementary/Corn_data/Combined_Final_Product.xlsx") 
names(dat)[c(10,8)] <- c("Kernel.color","Pop.structure")
Xind <- 11:ncol(dat)
foo <- dat[, c(10,8,Xind[-c(15,19,22,23,24)])] 
foo$Pop.structure <- factor(foo$Pop.structure) 
glmdr_model <- glmdr(Kernel.color~.,data=foo, family="binomial")
cases <- unique(foo[,2:ncol(foo)])
problematic_points <- cases[sapply(1:nrow(cases),function(i) nlevels(as.factor(
  inner_join(foo[,],cases[i,], by=names(cases))$Kernel.color))==1),] #line 70
identical(problematic_points, foo[!glmdr_model$linearity,-1])
```

After all, I wanted to find the way to express "easier" way of our method in high dimension case as it is quite hard to make the reader (who may not be familiar with complete separation) understand how our approach handles the complete separation (e.g., what is linearity & meaning of the null eigenvector of Fisher information). If we can argue that complete separation happens when there is no both response variable values (0,1) for each space divided by decision boundary (this is reason why I showed the contingency table in the logistc regression in the meeting), I think it is little bit easier to explain our method in complete separation. Furthermore we may connect complete separation to the incomplete tables (zeros in the cells) cases in Poisson regression (i.e., structural zeros is Poisson version of complete separation). But I am not entirely sure about this idea. We can just stick to the Agresit example and covered what we discussed previously. But, I thought it could be possible to provide eaiser explanation if 1 line of code (46 or 70th) can do the similar thing as glmdr.

## Fisher information

My idea was I wanted to develop our parer in terms of the Fisher information. For example, diagonal value of Fisher information shows the variance of score function of each predictor and for complete separation case, they are going to be small (or zero). 

* I am not entirely sure this is true but based on the Wikipedia (https://en.wikipedia.org/wiki/Fisher_information), "the Fisher information is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter of a distribution that models X" So, I thought in the Agresti example, X values less than 50 does not have any information on response variable 1 and X values larger than 50 does not have any information on response variable 0. Therefore, fisher information of X is 0 meaning X has no ability to estimate parameter (coefficient) and that is reason why MLE does not exist. Also, Wikipedia says "the Fisher information may be seen as the curvature of the support curve. Near the maximum likelihood estimate, low Fisher information therefore indicates that the maximum appears "blunt", that is, the maximum is shallow and there are many nearby values with a similar log-likelihood." So, I think blunt means here flat likelihood function (since it is at infinity). But, I am not sure if I totally misunderstood the concept here.

### Agresti example

In this example, the model is completely degenerated and MLE for x (and intercept) does not exist.
Therefore, **diagonal** elements of variance-covariance matrix of $\beta$ goes to infinity, which results in extremely small value in Fisher information.

```{r}
mod <- glmdr(y~x, family="binomial")
coef(summary(mod$om))
fish <- solve(vcov(mod$om))
fish
```

### Maize data

For Maize data, all coefficients of markers (starting with S6_) seem going to infinity. But their diagonal value of fisher information does not go to zero.

```{r}
mod <- glmdr(Kernel.color ~ ., data=foo,family="binomial")
coef(summary(mod$om))
diag(solve(vcov(mod$om)))
```

This is the same for the model without pop structure.

```{r}
mod <- glmdr(Kernel.color ~ .-Pop.structure, data=foo,family="binomial")
coef(summary(mod$om))
diag(solve(vcov(mod$om)))
```

So, I wonder this is because scale problem.
