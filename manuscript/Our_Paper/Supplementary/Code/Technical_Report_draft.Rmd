---
title: "Tehnical Reports"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE,message=FALSE}
library(readxl)
library(glmdr)
library(brglm2)
library(arm)
library(binom)
library(ggplot2)
library(gridExtra)
library(logistf)
library(foreach)
library(doParallel)
library(latex2exp)
library(PresenceAbsence)
numCores <- detectCores()
registerDoParallel(numCores)
```

# Agresti Complete Separation Example

## Motivation

```{r}
x <- c(10,20,30,40,60,70,80,90)
y <- c(0,0,0,0,1,1,1,1)
mod1 <- glm(y~x,family="binomial")
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x,family="binomial")
mod_CI <- inference(glmdr_mod)


plot1 <- ggplot(data=as.data.frame(cbind(x,y)),aes(x=x,y=y)) +
  geom_point(size=3) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))+
  scale_x_continuous(breaks = x)+
  labs(x="z",y="y")+
  scale_y_continuous(breaks = c(0.0,1.0))

plot2 <- ggplot(data=as.data.frame(cbind(x,y,mod_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = y, xend = x, yend = lower)) +
  geom_segment(aes(x = x, y = y, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="z", y= TeX('$\\textbf{\\hat{p}_x}$')) +
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

#ggsave("./plot1.png",plot1)
#ggsave("./One_sided_CI.png",plot2,device="png")

grid.arrange(plot1,plot2, ncol=2)

asymptote <- t(sapply(1:30, function(iter){
  m1 <- suppressWarnings(glm(y ~ x, family = "binomial", control = list(maxit = iter, epsilon = 1e-50)))
  c(log(sqrt(crossprod(coef(m1)))),logLik(m1))
  }))
asymptote <- as.data.frame(asymptote)
asymptote$V3 <- as.factor(ifelse(asymptote$V1 >=4.5,1,0))
plot3 <- ggplot(asymptote,aes(x = V1, y = V2)) +
  labs(x= expression(log(~"||"~beta~"||")), y= "log likelihood") +
  geom_line(col = "black") +
  geom_point(pch=1,col="blue",size=2) +
  geom_abline(intercept = 0, slope = 0, lty = 2, col = "red") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))

plot4 <- ggplot(asymptote[asymptote$V3 == 1,],aes(x = V1, y = V2)) +
  labs(x= expression(log(~"||"~beta~"||")), y= "log likelihood") +
  geom_line(col = "black") +
  xlim(4.5,5.0)+
  geom_point(pch=1,col="blue",size=2) +
  geom_abline(intercept = 0, slope = 0, lty = 2, col = "red") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))

grid.arrange(plot3,plot4, ncol=2)

```

### Comparison

```{r, warning=FALSE}
bayes_mod <- bayesglm(y~x,family="binomial")
glmdr_mod <- glmdr(y~x,family="binomial")  
brglm_mod <- glm(y~x, family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x)

glmdr_pred <- predict(glmdr_mod$om,type="response",se.fit=TRUE)
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_CI <- binom.confint(bayes_pred$fit,n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

brglm_pred <- predict(brglm_mod,type="response",se.fit=TRUE)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,brglm_length,lm_length) / length(x)
```

```{r}
mean(as.numeric(glmdr_pred$fit >= 0.5) == y)
mean(as.numeric(bayes_pred$fit >= 0.5) == y)
mean(as.numeric(brglm_pred$fit >= 0.5) == y)
mean(as.numeric(lm_pred$fit >= 0.5) == y)
```


## Prediction

```{r}
n_sample <- length(x)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x, data=training,family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc")
  ret2
}
total_mat_glmdr_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x, data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_bayes_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#brglm2
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(y~x, family = binomial(logit), data=training,  method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod,newdata=testing_X,type="response")
  ret2
}
total_mat_brglm_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_linear_test <- do.call(c,ret)
total_mat_linear_test <- ifelse(total_mat_linear_test>1,1,total_mat_linear_test)
total_mat_linear_test <- ifelse(total_mat_linear_test<0,0,total_mat_linear_test)
```

```{r, eval=FALSE}
df <- data.frame("ID"=1:8,"observed"=y,"predicted"=total_mat_glmdr_test)
optimal.thresholds(df)
```
```{r}
mean(as.numeric(total_mat_glmdr_test >= 0.5) == y)
mean(as.numeric(total_mat_bayes_test >= 0.5) == y)
mean(as.numeric(total_mat_brglm_test >= 0.5) == y)
mean(as.numeric(total_mat_linear_test >= 0.5) == y)
```

```{r}
glmdr_PI <- sum(apply(binom.confint(total_mat_glmdr_test,1,methods="wilson")[,c(5,6)],1,diff))
bayes_PI <- sum(apply(binom.confint(total_mat_bayes_test,1,methods="wilson")[,c(5,6)],1,diff))
brglm_PI <- sum(apply(binom.confint(total_mat_brglm_test,1,methods="wilson")[,c(5,6)],1,diff))
linear_PI <- sum(apply(binom.confint(total_mat_linear_test,1,methods="wilson")[,c(5,6)],1,diff))
ret <- c(glmdr_PI,bayes_PI,brglm_PI,linear_PI) / length(x)
names(ret) <- c("glmdr","Bayesglm","brglm","linear")
ret
```


# Agresti Quasi-Complete Separation Example

## Motivation

```{r}
x <- c(10,20,30,40,60,70,80,90,50,50)
y <- c(0,0,0,0,1,1,1,1,1,0)
mod1 <- glm(y~x,family="binomial")
summary(mod1)
plot(y~x, pch=16)
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x,family="binomial")
mod_CI <- inference(glmdr_mod)
mod_CI <- rbind(mod_CI,c(0,0.95),c(0.05,1))
plot(x, y, ylim = c(0,1), pch = 16, ylab = "", xlab = "")
points(x, mod_CI[, 1])
points(x, mod_CI[, 2])
segments(x, mod_CI[, 1], x, mod_CI[, 2])
```

### Comparison

```{r, warning=FALSE}
bayes_mod <- bayesglm(y~x,family="binomial")
glmdr_mod <- glmdr(y~x,family="binomial")  
brglm_mod <- glm(y~x, family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x)

glmdr_pred <- c(predict(glmdr_mod$om,type="response",se.fit=TRUE)$fit[!glmdr_mod$linearity],
                predict(glmdr_mod$lcm,type="response"))
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_CI <- binom.confint(bayes_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

brglm_pred <- predict(brglm_mod,type="response",se.fit=TRUE)
brglm_CI <- binom.confint(predict(brglm_mod,type="response")[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
lm_CI <- binom.confint(lm_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,brglm_length,lm_length) / sum(!glmdr_mod$linearity)
```

```{r}
my_dat <- data.frame(1:length(y),y,glmdr_pred,bayes_pred$fit,brglm_pred$fit,
                     lm_pred$fit)
optimal.thresholds(my_dat,opt.methods = "MaxPCC")
```


```{r}
mean(as.numeric(glmdr_pred >= 0.5) == y)
mean(as.numeric(bayes_pred$fit >= 0.5) == y)
mean(as.numeric(brglm_pred$fit >= 0.5) == y)
mean(as.numeric(lm_pred$fit >= 0.5) == y)
```


## Prediction

```{r}
n_sample <- length(x)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x, data=training,family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc")
  ret2
}
total_mat_glmdr_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x, data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_bayes_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(y~x, family = binomial(logit), data=training,  method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod,newdata=testing_X,type="response")
  ret2
}
total_mat_brglm_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_linear_test <- do.call(c,ret)
total_mat_linear_test <- ifelse(total_mat_linear_test <0,0,total_mat_linear_test)
total_mat_linear_test <- ifelse(total_mat_linear_test >1,1,total_mat_linear_test)
```


```{r, eval=FALSE}
my_dat <- data.frame(1:length(y),y,total_mat_glmdr_test,total_mat_bayes_test,total_mat_brglm_test,
                     total_mat_linear_test)
optimal.thresholds(my_dat,opt.methods = "MaxPCC")
```

```{r}
mean(as.numeric(total_mat_glmdr_test >= 0.5) == y)
mean(as.numeric(total_mat_bayes_test >= 0.5) == y)
mean(as.numeric(total_mat_brglm_test >= 0.5) == y)
mean(as.numeric(total_mat_linear_test >= 0.5) == y)
```

```{r}
glmdr_PI <- sum(apply(binom.confint(total_mat_glmdr_test,1,methods="wilson")[,c(5,6)],1,diff))
bayes_PI <- sum(apply(binom.confint(total_mat_bayes_test,1,methods="wilson")[,c(5,6)],1,diff))
brglm_PI <- sum(apply(binom.confint(total_mat_brglm_test,1,methods="wilson")[,c(5,6)],1,diff))
linear_PI <- sum(apply(binom.confint(total_mat_linear_test,1,methods="wilson")[,c(5,6)],1,diff))
ret <- c(glmdr_PI,bayes_PI,brglm_PI,linear_PI) / length(x)
names(ret) <- c("glmdr","Bayesglm","brglm","linear")
ret
```


# Quadratic Example

## Motivation

```{r}
data(quadratic)
mod <-  glm(y~x+I(x^2),data=quadratic,family="binomial")
summary(mod)
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x+I(x^2),data=quadratic,family="binomial")
mod_CI <- inference(glmdr_mod)
with(quadratic,plot(x, y, ylim = c(0,1), pch = 16, 
     ylab = "", xlab = ""))
with(quadratic,points(x, mod_CI[, 1]))
with(quadratic,points(x, mod_CI[, 2]))
with(quadratic,segments(x, mod_CI[, 1], x, mod_CI[, 2]))
```

### Comparison

```{r, warning=FALSE}
bayes_mod <- bayesglm(y~x+I(x^2),data=quadratic,family="binomial")
glmdr_mod <- glmdr(y~x+I(x^2),data=quadratic,family="binomial")  
brglm_mod <- glm(y~x+I(x^2),data=quadratic,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x+I(x^2),data=quadratic)  


glmdr_pred <- predict(glmdr_mod$om,type="response",se.fit=TRUE)
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_CI <- binom.confint(bayes_pred$fit,n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

brglm_pred <- predict(brglm_mod,type="response",se.fit=TRUE)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,brglm_length,lm_length) / nrow(quadratic)
```

```{r}
mean(as.numeric(glmdr_pred$fit >= 0.5) == quadratic$y)
mean(as.numeric(bayes_pred$fit >= 0.5) == quadratic$y)
mean(as.numeric(brglm_pred$fit >= 0.5) == quadratic$y)
mean(as.numeric(lm_pred$fit >= 0.5) == quadratic$y)
```

## Prediction

```{r}
n_sample <- nrow(quadratic)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x+I(x^2),data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc")
  ret2
}
total_mat_glmdr_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x+I(x^2),data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_bayes_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(y~x+I(x^2), family = binomial(logit), data=training,  method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod,newdata=testing_X,type="response")
  ret2
}
total_mat_brglm_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x+I(x^2),data=training)
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_linear_test <- do.call(c,ret)
total_mat_linear_test <- ifelse(total_mat_linear_test <0,0,total_mat_linear_test)
total_mat_linear_test <- ifelse(total_mat_linear_test >1,1,total_mat_linear_test)
```

```{r, eval=FALSE}
my_dat <- data.frame(1:length(quadratic$y),quadratic$y,
                     total_mat_glmdr_test,total_mat_bayes_test,total_mat_brglm_test,
                     total_mat_linear_test)
optimal.thresholds(my_dat,opt.methods = "MaxPCC")
```

```{r}
mean(as.numeric(total_mat_glmdr_test >= 0.5) == quadratic$y)
mean(as.numeric(total_mat_bayes_test >= 0.57) == quadratic$y)
mean(as.numeric(total_mat_brglm_test >= 0.62) == quadratic$y)
mean(as.numeric(total_mat_linear_test >= 0.575) == quadratic$y)
```

```{r}
glmdr_PI <- sum(apply(binom.confint(total_mat_glmdr_test,1,methods="wilson")[,c(5,6)],1,diff))
bayes_PI <- sum(apply(binom.confint(total_mat_bayes_test,1,methods="wilson")[,c(5,6)],1,diff))
brglm_PI <- sum(apply(binom.confint(total_mat_brglm_test,1,methods="wilson")[,c(5,6)],1,diff))
linear_PI <- sum(apply(binom.confint(total_mat_linear_test,1,methods="wilson")[,c(5,6)],1,diff))
ret <- c(glmdr_PI,bayes_PI,brglm_PI,linear_PI) /  nrow(quadratic)
names(ret) <- c("glmdr","Bayesglm","brglm","linear")
ret
```

# Endometrial

## Motivation
```{r}
data(endometrial)
mod <- glm(HG~.,data=endometrial,family="binomial")
summary(mod)
with(endometrial,plot(HG~NV,pch=16))
```

## Estimation

```{r}
glmdr_mod <- glmdr(HG~.,data=endometrial,family="binomial")
inference(glmdr_mod)
```


## Comparison

```{r, warning=FALSE}
bayes_mod <- bayesglm(HG~.,data=endometrial,family="binomial")
glmdr_mod <- glmdr(HG~.,data=endometrial,family="binomial")  
brglm_mod <- glm(HG~.,data=endometrial,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(HG~.,data=endometrial)  

glmdr_pred <- c(predict(glmdr_mod$om,type="response",se.fit=TRUE)$fit[!glmdr_mod$linearity],
                predict(glmdr_mod$lcm,type="response"))
glmdr_pred <- glmdr_pred[order(as.numeric(names(glmdr_pred)))]
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_CI <- binom.confint(bayes_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

brglm_pred <- predict(brglm_mod,type="response",se.fit=TRUE)
brglm_CI <- binom.confint(predict(brglm_mod,type="response")[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
lm_CI <- binom.confint(lm_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,brglm_length,lm_length) / sum(!glmdr_mod$linearity)
```

```{r}
mean(as.numeric(glmdr_pred >= 0.5) == endometrial$HG)
mean(as.numeric(bayes_pred$fit >= 0.5) == endometrial$HG)
mean(as.numeric(brglm_pred$fit >= 0.5) == endometrial$HG)
mean(as.numeric(lm_pred$fit >= 0.5) == endometrial$HG)
```


## Prediction

```{r}
n_sample <- nrow(endometrial)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(HG~.,data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc")
  ret2
}
total_mat_glmdr_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(HG~.,data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_bayes_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#brglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(HG~.,data=training,family = binomial(logit), method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_brglm_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(HG~.,data=training)
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_linear_test <- do.call(c,ret)
total_mat_linear_test <- ifelse(total_mat_linear_test <0,0,total_mat_linear_test)
total_mat_linear_test <- ifelse(total_mat_linear_test >1,1,total_mat_linear_test)
```

```{r, eval=FALSE}
my_dat <- data.frame(1:nrow(endometrial),endometrial$HG,
                     total_mat_glmdr_test,total_mat_bayes_test,total_mat_brglm_test,
                     total_mat_linear_test)
optimal.thresholds(my_dat,opt.methods = "MaxPCC")
```

```{r}
mean(as.numeric(total_mat_glmdr_test >= 0.483) == endometrial$HG)
mean(as.numeric(total_mat_bayes_test >= 0.47) == endometrial$HG)
mean(as.numeric(total_mat_brglm_test >= 0.47) == endometrial$HG)
mean(as.numeric(total_mat_linear_test >= 0.44) == endometrial$HG)
```

```{r}
glmdr_PI <- sum(apply(binom.confint(total_mat_glmdr_test,1,methods="wilson")[,c(5,6)],1,diff))
bayes_PI <- sum(apply(binom.confint(total_mat_bayes_test,1,methods="wilson")[,c(5,6)],1,diff))
brglm_PI <- sum(apply(binom.confint(total_mat_brglm_test,1,methods="wilson")[,c(5,6)],1,diff))
linear_PI <- sum(apply(binom.confint(total_mat_linear_test,1,methods="wilson")[,c(5,6)],1,diff))
ret <- c(glmdr_PI,bayes_PI,brglm_PI,linear_PI) / nrow(endometrial)
names(ret) <- c("glmdr","Bayesglm","brglm","linear")
ret
```

# Maize data

## Data Cleaning

```{r}
corn_dat <- readxl::read_xlsx("./Combined_Final_Product.xlsx")
names(corn_dat)[c(10,8)] <- c("Kernel.color","Pop.structure")
Xind <- 11:ncol(corn_dat)
foo <- corn_dat[, c(10,8,Xind[-c(15,19,22,23,24)])] 
foo$Pop.structure <- factor(foo$Pop.structure) 
dat <- data.frame(foo)
dat[,-c(1,2)] <- scale(dat[,-c(1,2)]) #scale the data
```

## Motivation

### Univariate Case

When we fit the logstic model one by one (univariate case), there is no separation.

```{r}
n <- ncol(dat)
ret_vec <- rep(TRUE,n) #TRUE: separation does not exist | FALSE: spearation exists
Y <- foo$Kernel.color
for(i in 2:n){
  X <- unlist(dat[,i])
  mod <- glmdr(Y ~ X, family="binomial")
  ret_vec[(i-1)] <- is.null(mod$linearity)
}
all(ret_vec)
```

### Multivariate Case

```{r}
mod <- glm(Kernel.color~.,data=dat,family="binomial")
summary(mod)
```

## Estimation

```{r}
glmdr_mod <- glmdr(Kernel.color~.,data=dat,family="binomial")
inference(glmdr_mod)
```

### Comparison

```{r}
bayes_mod <- bayesglm(Kernel.color~.,data=dat,family="binomial")
glmdr_mod <- glmdr(Kernel.color~.,data=dat,family="binomial")  
logistif_mod <- logistf(Kernel.color~.,data=dat, family = "binomial")
lm_mod <- lm(Kernel.color~.,data=dat)

glmdr_pred <- c(predict(glmdr_mod$om,type="response",se.fit=TRUE)$fit[!glmdr_mod$linearity],
                predict(glmdr_mod$lcm,type="response"))
glmdr_pred <- glmdr_pred[order(as.numeric(names(glmdr_pred)))]
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_CI <- binom.confint(bayes_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

logistif_pred <- c(invlogit(model.matrix(glmdr_mod$om) %*% logistif_mod$coefficients))
logistf_CI <- binom.confint(logistif_pred[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
logistf_length <- sum(as.matrix(logistf_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
lm_CI <- binom.confint(lm_pred$fit[!glmdr_mod$linearity],n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,logistf_length,lm_length) / sum(!glmdr_mod$linearity)
```

```{r}
mean(as.numeric(glmdr_pred >= 0.5) == dat$Kernel.color)
mean(as.numeric(bayes_pred$fit >= 0.5) == dat$Kernel.color)
mean(as.numeric(logistif_pred >= 0.5) == dat$Kernel.color)
mean(as.numeric(lm_pred$fit >= 0.5) == dat$Kernel.color)
```


## Prediction & Comparison


```{r, eval=FALSE}
n_sample <- nrow(dat)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc")
  ret2
}
total_mat_glmdr_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_bayes_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  logistf_mod <- logistf(Kernel.color~.,data=training, family="binomial")
  new_dat <- rbind(logistf_mod$data,as.data.frame(cbind("Kernel.color"=testing_Y,testing_X),col.names=colnames(logistf_mod$data)))
  ret2 <- c(invlogit(tail(model.matrix(Kernel.color~.,data=new_dat),1) %*% logistf_mod$coefficients))
  ret2
}
total_mat_logistf_test <- do.call(c,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  ret2
}
total_mat_linear_test <- do.call(c,ret)
total_mat_linear_test <- ifelse(total_mat_linear_test <0,0,total_mat_linear_test)
total_mat_linear_test <- ifelse(total_mat_linear_test >1,1,total_mat_linear_test)
```

```{r}
#total_mat_glmdr_test <- read.csv("../../../../../../Data/glmdr_predicted_ret.csv")[,-1]
#total_mat_bayes_test <- read.csv("../../../../../../Data/bayes_predicted_ret.csv")[,-1]
total_mat_logistf_test <- read.csv("../../../../../../Data/logistf_predicted_ret.csv")[,-1]
#total_mat_linear_test <- read.csv("../../../../../../Data/linear_predicted_ret.csv")[,-1]
```


```{r, eval=FALSE}
my_dat <- data.frame(1:nrow(dat),dat$Kernel.color,
                     total_mat_glmdr_test,total_mat_bayes_test,total_mat_logistf_test,
                     total_mat_linear_test)
optimal.thresholds(my_dat,opt.methods = "MaxPCC")
```

```{r}
mean(as.numeric(total_mat_glmdr_test >= 0.68) == dat$Kernel.color)
mean(as.numeric(total_mat_bayes_test >= 0.62) == dat$Kernel.color)
mean(as.numeric(total_mat_logistf_test >= 0.85) == dat$Kernel.color)
mean(as.numeric(total_mat_linear_test >= 0.605) == dat$Kernel.color)
```

```{r}
glmdr_PI <- sum(apply(binom.confint(total_mat_glmdr_test,1,methods="wilson")[,c(5,6)],1,diff))
bayes_PI <- sum(apply(binom.confint(total_mat_bayes_test,1,methods="wilson")[,c(5,6)],1,diff))
brglm_PI <- sum(apply(binom.confint(total_mat_logistf_test,1,methods="wilson")[,c(5,6)],1,diff))
linear_PI <- sum(apply(binom.confint(total_mat_linear_test,1,methods="wilson")[,c(5,6)],1,diff))
ret <- c(glmdr_PI,bayes_PI,brglm_PI,linear_PI) / nrow(dat)
names(ret) <- c("glmdr","Bayesglm","logistf","linear")
ret
```

