---
title: "Tehnical Reports"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE,message=FALSE}
library(readxl)
library(glmdr)
library(brglm2)
library(arm)
library(binom)
library(ggplot2)
library(gridExtra)
library(logistf)
library(foreach)
library(doParallel)
library(latex2exp)
```

# Agresti Complete Separation Example

## Motivation

```{r}
x <- c(10,20,30,40,60,70,80,90)
y <- c(0,0,0,0,1,1,1,1)
mod1 <- glm(y~x,family="binomial")
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x,family="binomial")
mod_CI <- inference(glmdr_mod)


plot1 <- ggplot(data=as.data.frame(cbind(x,y)),aes(x=x,y=y)) +
  geom_point(size=3) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))+
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

plot2 <- ggplot(data=as.data.frame(cbind(x,y,mod_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = y, xend = x, yend = lower)) +
  geom_segment(aes(x = x, y = y, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="x", y= TeX('$\\textbf{\\hat{p}}$')) +
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

#ggsave("./plot1.png",plot1)
#ggsave("./One_sided_CI.png",plot2,device="png")

grid.arrange(plot1,plot2, ncol=2)

asymptote <- t(sapply(1:30, function(iter){
  m1 <- suppressWarnings(glm(y ~ x, family = "binomial", control = list(maxit = iter, epsilon = 1e-50)))
  c(log(sqrt(crossprod(coef(m1)))),logLik(m1))
  }))
asymptote <- as.data.frame(asymptote)
asymptote$V3 <- as.factor(ifelse(asymptote$V1 >=4.5,1,0))
plot3 <- ggplot(asymptote,aes(x = V1, y = V2)) +
  labs(x= expression(log(~"||"~beta~"||")), y= "log likelihood") +
  geom_line(col = "black") +
  geom_point(pch=1,col="blue",size=2) +
  geom_abline(intercept = 0, slope = 0, lty = 2, col = "red") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))

plot4 <- ggplot(asymptote[asymptote$V3 == 1,],aes(x = V1, y = V2)) +
  labs(x= expression(log(~"||"~beta~"||")), y= "log likelihood") +
  geom_line(col = "black") +
  xlim(4.5,5.0)+
  geom_point(pch=1,col="blue",size=2) +
  geom_abline(intercept = 0, slope = 0, lty = 2, col = "red") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))

grid.arrange(plot3,plot4, ncol=2)

```

### Comparison

```{r}
bayes_mod <- bayesglm(y~x,family="binomial")
glmdr_mod <- glmdr(y~x,family="binomial")  
brglm_mod <- glm(y~x, family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x)

mean((predict(bayes_mod,type="response") >= 0.5) == y)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))


mean((predict(bayes_mod,type="response") >= 0.5) == quadratic$y)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_pred <- predict(bayes_mod,type="response",se.fit=TRUE)
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))


mean((predict(brglm_mod,type="response") >= 0.5) == y)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

mean((predict(glmdr_mod$om,type="response") >= 0.5) == y)
glmdr_CI <- inference(glmdr_mod)
glmdr_pred <- predict(glmdr_mod$om,type="response",se.fit=TRUE)
glmdr_CI <- cbind(glmdr_pred$fit - 1.96 * glmdr_pred$se.fit,glmdr_pred$fit + 1.96 * glmdr_pred$se.fit)
glmdr_CI <- binom.confint(predict(glmdr_mod$om,type="response"),n=1,methods="wilson")[,c(5,6)]

glmdr_CI <- 
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
mean((lm_pred$fit >= 0.5) == y)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


p1 <- ggplot(data=as.data.frame(cbind(x,y,glmdr_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = y, xend = x, yend = lower)) +
  geom_segment(aes(x = x, y = y, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="x", y= TeX('$\\textbf{\\hat{p}}$')) +
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

p2 <- ggplot(data=as.data.frame(cbind(x,y,glmdr_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = y, xend = x, yend = lower)) +
  geom_segment(aes(x = x, y = y, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="x", y= TeX('$\\textbf{\\hat{p}}$')) +
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

p3 <- ggplot(data=as.data.frame(cbind(x,y,glmdr_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = y, xend = x, yend = lower)) +
  geom_segment(aes(x = x, y = y, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="x", y= TeX('$\\textbf{\\hat{p}}$')) +
  scale_x_continuous(breaks = x)+
  scale_y_continuous(breaks = c(0.0,1.0))

p4 <- ggplot(data=as.data.frame(cbind(x,y,lm_CI)),aes(x=x,y=y)) +
  geom_point(size=3) +
  geom_point(aes(x=x, y=lower),size=3,pch=1) +
  geom_point(aes(x=x, y=upper),size=3,pch=1) +
  geom_point(size=3) +
  geom_segment(aes(x = x, y = lower, xend = x, yend = upper)) +
  theme(panel.background = element_rect(fill = "white", colour = "grey50"), axis.title=element_text(size=15))+
  labs(x="x", y= TeX('$\\textbf{\\hat{p}}$')) +
  scale_x_continuous(breaks = x)


cbind(glmdr_length,bayes_length,brglm_length,lm_length) / length(x)
```

```{r}
plot(lm_mod)
```


## Prediction

```{r}
n_sample <- length(x)
total_mat_glmdr_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_bayes_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_logistf_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_linear_test <- matrix(NA,nrow=n_sample,ncol=4)
numCores <- detectCores()
registerDoParallel(numCores)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x, data=training,family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc",interval = "wilson")
  total_mat_glmdr_test[i,1] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 1) #true positive
  total_mat_glmdr_test[i,2] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 0) #true negative
  total_mat_glmdr_test[i,3] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 0) #false positive
  total_mat_glmdr_test[i,4] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 1) #false negative
  total_mat_glmdr_test[i,]
}
total_mat_glmdr_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x, data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  total_mat_bayes_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_bayes_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_bayes_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_bayes_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_bayes_test[i,]
}
total_mat_bayes_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  logistf_mod <- logistf(y~x,data=training, family="binomial")
  new_dat <- rbind(logistf_mod$data,as.data.frame(cbind("y"=testing_Y,testing_X),col.names=colnames(logistf_mod$data)))
  ret2 <- c(invlogit(tail(model.matrix(y~.,data=new_dat),1) %*% logistf_mod$coefficients))
  total_mat_logistf_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_logistf_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_logistf_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_logistf_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_logistf_test[i,]
}
total_mat_logistf_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  total_mat_linear_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_linear_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_linear_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_linear_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_linear_test[i,]
}
total_mat_linear_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
sum(colSums(total_mat_glmdr_test[,1:4])[1:2]) / sum(colSums(total_mat_glmdr_test[,1:4])) # glmdr

sum(colSums(total_mat_bayes_test[,1:4])[1:2]) / sum(colSums(total_mat_bayes_test[,1:4])) # bayesglm

sum(colSums(total_mat_logistf_test[,1:4])[1:2]) / sum(colSums(total_mat_logistf_test[,1:4])) # logistf

sum(colSums(total_mat_linear_test[,1:4])[1:2]) / sum(colSums(total_mat_linear_test[,1:4])) # linear
```


# Agresti Quasi-Complete Separation Example

## Motivation

```{r}
x <- c(10,20,30,40,60,70,80,90,50,50)
y <- c(0,0,0,0,1,1,1,1,0,1)
mod1 <- glm(y~x,family="binomial")
summary(mod1)
plot(y~x, pch=16)
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x,family="binomial")
mod_CI <- inference(glmdr_mod)
mod_CI <- rbind(mod_CI,c(0,0.95),c(0.05,1))
plot(x, y, ylim = c(0,1), pch = 16, ylab = "", xlab = "")
points(x, mod_CI[, 1])
points(x, mod_CI[, 2])
segments(x, mod_CI[, 1], x, mod_CI[, 2])
```

### Comparison

```{r}
bayes_mod <- bayesglm(y~x,family="binomial")
glmdr_mod <- glmdr(y~x,family="binomial")  
brglm_mod <- glm(y~x, family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x)

mean((predict(bayes_mod,type="response") >= 0.5) == y)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

mean((predict(brglm_mod,type="response") >= 0.5) == y)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

mean(c((predict(glmdr_mod$om,type="response")[!glmdr_mod$linearity] >= 0.5) == y[!glmdr_mod$linearity]
       ,(predict(glmdr_mod$lcm,type="response") >= 0.5) == y[glmdr_mod$linearity]))
glmdr_CI <- inference(glmdr_mod)
glmdr_CI <- rbind(glmdr_CI,binom.confint(predict(glmdr_mod$lcm,type="response"),n=1,methods="wilson")[,c(5,6)])
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
mean((lm_pred$fit >= 0.5) == y)
lm_CI <- cbind(lm_pred$fit - (1.96 * lm_pred$se.fit),lm_pred$fit + (1.96 * lm_pred$se.fit))
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
mean((lm_pred$fit >= 0.5) == y)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

cbind(glmdr_length,bayes_length,brglm_length,lm_length) / length(x)
```
## Prediction

```{r}
n_sample <- length(x)
total_mat_glmdr_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_bayes_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_logistf_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_linear_test <- matrix(NA,nrow=n_sample,ncol=4)
numCores <- detectCores()
registerDoParallel(numCores)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x, data=training,family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc",interval = "wilson")
  total_mat_glmdr_test[i,1] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 1) #true positive
  total_mat_glmdr_test[i,2] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 0) #true negative
  total_mat_glmdr_test[i,3] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 0) #false positive
  total_mat_glmdr_test[i,4] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 1) #false negative
  total_mat_glmdr_test[i,]
}
total_mat_glmdr_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x, data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  total_mat_bayes_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_bayes_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_bayes_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_bayes_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_bayes_test[i,]
}
total_mat_bayes_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  logistf_mod <- logistf(y~x,data=training, family="binomial")
  new_dat <- rbind(logistf_mod$data,as.data.frame(cbind("y"=testing_Y,testing_X),col.names=colnames(logistf_mod$data)))
  ret2 <- c(invlogit(tail(model.matrix(y~.,data=new_dat),1) %*% logistf_mod$coefficients))
  total_mat_logistf_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_logistf_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_logistf_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_logistf_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_logistf_test[i,]
}
total_mat_logistf_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  total_mat_linear_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_linear_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_linear_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_linear_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_linear_test[i,]
}
total_mat_linear_test <- do.call(rbind,ret)
```


```{r, eval=FALSE}
sum(colSums(total_mat_glmdr_test[,1:4])[1:2]) / sum(colSums(total_mat_glmdr_test[,1:4])) # glmdr

sum(colSums(total_mat_bayes_test[,1:4])[1:2]) / sum(colSums(total_mat_bayes_test[,1:4])) # bayesglm

sum(colSums(total_mat_logistf_test[,1:4])[1:2]) / sum(colSums(total_mat_logistf_test[,1:4])) # logistf

sum(colSums(total_mat_linear_test[,1:4])[1:2]) / sum(colSums(total_mat_linear_test[,1:4])) # linear
```

# Quadratic Example

## Motivation

```{r}
data(quadratic)
mod <-  glm(y~x+I(x^2),data=quadratic,family="binomial")
summary(mod)
```

## Estimation

```{r}
glmdr_mod <- glmdr(y~x+I(x^2),data=quadratic,family="binomial")
mod_CI <- inference(glmdr_mod)
with(quadratic,plot(x, y, ylim = c(0,1), pch = 16, 
     ylab = "", xlab = ""))
with(quadratic,points(x, mod_CI[, 1]))
with(quadratic,points(x, mod_CI[, 2]))
with(quadratic,segments(x, mod_CI[, 1], x, mod_CI[, 2]))
```

### Comparison

```{r}
bayes_mod <- bayesglm(y~x+I(x^2),data=quadratic,family="binomial")
glmdr_mod <- glmdr(y~x+I(x^2),data=quadratic,family="binomial")  
brglm_mod <- glm(y~x+I(x^2),data=quadratic,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(y~x+I(x^2),data=quadratic)  

mean((predict(bayes_mod,type="response") >= 0.5) == quadratic$y)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_pred <- predict(bayes_mod,se.fit=TRUE)
bayes_CI_Wald <- invlogit(cbind(bayes_pred$fit - 1.96 * bayes_pred$se.fit,bayes_pred$fit + 1.96 * bayes_pred$se.fit))
sum(apply(bayes_CI_Wald,1,diff))
sum(apply(bayes_CI,1,diff))
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

mean((predict(brglm_mod,type="response") >= 0.5) == quadratic$y)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

mean((predict(glmdr_mod$om,type="response") >= 0.5) == quadratic$y)
glmdr_CI <- inference(glmdr_mod)
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
mean((lm_pred$fit >= 0.5) == quadratic$y)
lm_CI <- cbind(lm_pred$fit - (1.96 * lm_pred$se.fit),lm_pred$fit + (1.96 * lm_pred$se.fit))
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
mean((lm_pred$fit >= 0.5) == quadratic$y)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


cbind(glmdr_length,bayes_length,brglm_length,lm_length) / nrow(quadratic)
```

## Prediction

```{r}
n_sample <- nrow(quadratic)
total_mat_glmdr_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_bayes_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_logistf_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_linear_test <- matrix(NA,nrow=n_sample,ncol=4)
numCores <- detectCores()
registerDoParallel(numCores)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(y~x+I(x^2),data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc",interval = "wilson")
  total_mat_glmdr_test[i,1] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 1) #true positive
  total_mat_glmdr_test[i,2] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 0) #true negative
  total_mat_glmdr_test[i,3] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 0) #false positive
  total_mat_glmdr_test[i,4] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 1) #false negative
  total_mat_glmdr_test[i,]
}
total_mat_glmdr_test <- do.call(rbind,ret)
sum(colSums(total_mat_glmdr_test[,1:4])[1:2]) / sum(colSums(total_mat_glmdr_test[,1:4])) # glmdr
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(y~x+I(x^2),data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  total_mat_bayes_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_bayes_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_bayes_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_bayes_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_bayes_test[i,]
}
total_mat_bayes_test <- do.call(rbind,ret)

```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(y~x+I(x^2),data=training,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod, newdata= testing_X, type="response")
  total_mat_logistf_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_logistf_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_logistf_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_logistf_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_logistf_test[i,]
}
total_mat_logistf_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(y~x+I(x^2),data=training)
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  total_mat_linear_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_linear_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_linear_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_linear_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_linear_test[i,]
}
total_mat_linear_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
sum(colSums(total_mat_glmdr_test[,1:4])[1:2]) / sum(colSums(total_mat_glmdr_test[,1:4])) # glmdr

sum(colSums(total_mat_bayes_test[,1:4])[1:2]) / sum(colSums(total_mat_bayes_test[,1:4])) # bayesglm

sum(colSums(total_mat_logistf_test[,1:4])[1:2]) / sum(colSums(total_mat_logistf_test[,1:4])) # logistf

sum(colSums(total_mat_linear_test[,1:4])[1:2]) / sum(colSums(total_mat_linear_test[,1:4])) # linear
```

# Endometrial

## Motivation
```{r}
data(endometrial)
mod <- glm(HG~.,data=endometrial,family="binomial")
summary(mod)
with(endometrial,plot(HG~NV,pch=16))
```

## Estimation

```{r}
glmdr_mod <- glmdr(HG~.,data=endometrial,family="binomial")
inference(glmdr_mod)
```


## Comparison

```{r}
bayes_mod <- bayesglm(HG~.,data=endometrial,family="binomial")
glmdr_mod <- glmdr(HG~.,data=endometrial,family="binomial")  
brglm_mod <- glm(HG~.,data=endometrial,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
lm_mod <- lm(HG~.,data=endometrial)  

mean((predict(bayes_mod,type="response") >= 0.5) == endometrial$HG)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_pred <- predict(bayes_mod,se.fit=TRUE)
bayes_CI_Wald <- cbind(bayes_pred$fit - 1.96 * bayes_pred$se.fit,bayes_pred$fit + 1.96 * bayes_pred$se.fit)
bayes_CI_Wald <- invlogit(bayes_CI_Wald)
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

mean((predict(brglm_mod,type="response") >= 0.5) == endometrial$HG)
brglm_CI <- binom.confint(predict(brglm_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
brglm_length <- sum(as.matrix(brglm_CI) %*% c(-1,1))

mean(c((predict(glmdr_mod$om,type="response")[!glmdr_mod$linearity] >= 0.5) == endometrial$HG[!glmdr_mod$linearity]
       ,(predict(glmdr_mod$lcm,type="response") >= 0.5) == endometrial$HG[glmdr_mod$linearity]))
glmdr_CI <- inference(glmdr_mod)
glmdr_CI <- rbind(glmdr_CI,binom.confint(predict(glmdr_mod$lcm,type="response"),n=1,methods="wilson")[,c(5,6)])
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

glmdr_CI_Wilson <- binom.confint(predict(glmdr_mod$lcm,type="response"),n=1,methods="wilson")[,c(5,6)]
glmdr_pred <- predict(glmdr_mod$om,se.fit=TRUE)
glmdr_CI_Wald <- invlogit(cbind(glmdr_pred$fit - 1.96 * glmdr_pred$se.fit,glmdr_pred$fit + 1.96 * glmdr_pred$se.fit))

sum(apply(glmdr_CI_Wald,1,diff))
sum(apply(glmdr_CI_Wilson,1,diff))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
mean((lm_pred$fit >= 0.5) == endometrial$HG)
lm_CI <- cbind(lm_pred$fit - (1.96 * lm_pred$se.fit),lm_pred$fit + (1.96 * lm_pred$se.fit))
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
mean((lm_pred$fit >= 0.5) == endometrial$HG)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


cbind(glmdr_length,bayes_length,brglm_length,lm_length) /nrow(endometrial)
```
## Prediction

```{r}
n_sample <- nrow(endometrial)
total_mat_glmdr_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_bayes_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_logistf_test <- matrix(NA,nrow=n_sample,ncol=4)
total_mat_linear_test <- matrix(NA,nrow=n_sample,ncol=4)
numCores <- detectCores()
registerDoParallel(numCores)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
dat <- glmdr_mod$om$model
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(HG~.,data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc",interval = "wilson")
  total_mat_glmdr_test[i,1] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 1) #true positive
  total_mat_glmdr_test[i,2] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) == testing_Y & testing_Y == 0) #true negative
  total_mat_glmdr_test[i,3] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 0) #false positive
  total_mat_glmdr_test[i,4] <- sum(ifelse(rowMeans(ret2)>=0.5,1,0) != testing_Y & testing_Y == 1) #false negative
  total_mat_glmdr_test[i,]
}
total_mat_glmdr_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(HG~.,data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  total_mat_bayes_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_bayes_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_bayes_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_bayes_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_bayes_test[i,]
}
total_mat_bayes_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#logistf
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  brglm_mod <- glm(HG~.,data=training,family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
  ret2 <- predict(brglm_mod, newdata= testing_X, type="response")
  total_mat_logistf_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_logistf_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_logistf_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_logistf_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_logistf_test[i,]
}
total_mat_logistf_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(HG~.,data=training)
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  total_mat_linear_test[i,1] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_linear_test[i,2] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_linear_test[i,3] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_linear_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative
  total_mat_linear_test[i,]
}
total_mat_linear_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
sum(colSums(total_mat_glmdr_test[,1:4])[1:2]) / sum(colSums(total_mat_glmdr_test[,1:4])) # glmdr

sum(colSums(total_mat_bayes_test[,1:4])[1:2]) / sum(colSums(total_mat_bayes_test[,1:4])) # bayesglm

sum(colSums(total_mat_logistf_test[,1:4])[1:2]) / sum(colSums(total_mat_logistf_test[,1:4])) # logistf

sum(colSums(total_mat_linear_test[,1:4])[1:2]) / sum(colSums(total_mat_linear_test[,1:4])) # linear
```

# Maize data

## Data Cleaning

```{r}
corn_dat <- readxl::read_xlsx("./Combined_Final_Product.xlsx")
names(corn_dat)[c(10,8)] <- c("Kernel.color","Pop.structure")
Xind <- 11:ncol(corn_dat)
foo <- corn_dat[, c(10,8,Xind[-c(15,19,22,23,24)])] 
foo$Pop.structure <- factor(foo$Pop.structure) 
dat <- data.frame(foo)
dat[,-c(1,2)] <- scale(dat[,-c(1,2)]) #scale the data
```

## Motivation

### Univariate Case

When we fit the logstic model one by one (univariate case), there is no separation.

```{r}
n <- ncol(dat)
ret_vec <- rep(TRUE,n) #TRUE: separation does not exist | FALSE: spearation exists
Y <- foo$Kernel.color
for(i in 2:n){
  X <- unlist(dat[,i])
  mod <- glmdr(Y ~ X, family="binomial")
  ret_vec[(i-1)] <- is.null(mod$linearity)
}
all(ret_vec)
```

### Multivariate Case

```{r}
mod <- glm(Kernel.color~.,data=dat,family="binomial")
summary(mod)
```

## Estimation

```{r}
glmdr_mod <- glmdr(Kernel.color~.,data=dat,family="binomial")
inference(glmdr_mod)
```

### Comparison

```{r}
bayes_mod <- bayesglm(Kernel.color~.,data=dat,family="binomial")
glmdr_mod <- glmdr(Kernel.color~.,data=dat,family="binomial")  
#brglm_mod <- glm(Kernel.color~.,data=dat, family = "binomial", method = "brglmFit", type = "MPL_Jeffreys")
#brglm does not converge
logistif_mod <- logistf(Kernel.color~.,data=dat, family = "binomial")
lm_mod <- lm(Kernel.color~.,data=dat)

mean((predict(bayes_mod,type="response") >= 0.5) == dat$Kernel.color)
bayes_CI <- binom.confint(predict(bayes_mod,type="response"),n=1,methods="wilson")[,c(5,6)]
bayes_length <- sum(as.matrix(bayes_CI) %*% c(-1,1))

logistif_pred <- c(invlogit(model.matrix(glmdr_mod$om) %*% logistif_mod$coefficients))
mean((logistif_pred >= 0.5) == dat$Kernel.color)
logistf_CI <- binom.confint(logistif_pred,n=1,methods="wilson")[,c(5,6)]
logistf_length <- sum(as.matrix(logistf_CI) %*% c(-1,1))

mean(c((predict(glmdr_mod$om,type="response") >= 0.5)[!glmdr_mod$linearity] == dat$Kernel.color[!glmdr_mod$linearity],(predict(glmdr_mod$lcm,type="response") >= 0.5) == dat$Kernel.color[glmdr_mod$linearity]))
glmdr_CI <- inference(glmdr_mod)
glmdr_CI <- rbind(glmdr_CI,binom.confint(predict(glmdr_mod$lcm,type="response"),n=1,methods="wilson")[,c(5,6)])
glmdr_length <- sum(as.matrix(glmdr_CI) %*% c(-1,1))

lm_pred <- predict(lm_mod,se.fit=TRUE)
mean( (lm_pred$fit >= 0.5) == dat$Kernel.color)
lm_CI <- cbind(lm_pred$fit - (1.96 * lm_pred$se.fit),lm_pred$fit + (1.96 * lm_pred$se.fit))
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))

lm_pred <- (predict(lm_mod,se.fit = TRUE))
lm_pred$fit <- ifelse(lm_pred$fit<=0, 0,lm_pred$fit)
lm_pred$fit <- ifelse(lm_pred$fit>=1, 1,lm_pred$fit)
mean((lm_pred$fit >= 0.5) == dat$Kernel.color)
lm_CI <- binom.confint(lm_pred$fit,n=1,methods="wilson")[,c(5,6)]
lm_length <- sum(as.matrix(lm_CI) %*% c(-1,1))


cbind(glmdr_length,bayes_length,logistf_length,lm_length) / nrow(dat)
```

## Prediction & Comparison


```{r, eval=FALSE}
n_sample <- nrow(dat)
total_mat_glmdr_test <- matrix(NA,nrow=n_sample,ncol=8)
total_mat_bayes_test <- matrix(NA,nrow=n_sample,ncol=8)
total_mat_logistf_test <- matrix(NA,nrow=n_sample,ncol=8)
total_mat_linear_test <- matrix(NA,nrow=n_sample,ncol=8)

numCores <- detectCores()
registerDoParallel(numCores)
y_idx <- which(names(glmdr_mod$om$model) == paste(formula(glmdr_mod$om))[2])
#glmdr
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  glmdr_mod <- glmdr(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(glmdr_mod,newdata = testing_X, alpha=0.05,crit="AICc",interval = "wilson")
  total_mat_glmdr_test[i,1] <- i
  total_mat_glmdr_test[i,4] <- sum(ifelse(rowMeans(ret2)>0.5,1,0) == testing_Y & testing_Y == 1) #true positive
  total_mat_glmdr_test[i,5] <- sum(ifelse(rowMeans(ret2)>0.5,1,0) == testing_Y & testing_Y == 0) #true negative
  total_mat_glmdr_test[i,6] <- sum(ifelse(rowMeans(ret2)>0.5,1,0) != testing_Y & testing_Y == 0) #false positive
  total_mat_glmdr_test[i,7] <- sum(ifelse(rowMeans(ret2)>0.5,1,0) != testing_Y & testing_Y == 1) #false negative
  total_mat_glmdr_test[i,8] <- sum(total_mat_glmdr_test[i,4:7])
  total_mat_glmdr_test[i,]
}
total_mat_glmdr_test <- do.call(rbind,ret)

sum(colSums(total_mat_glmdr_test[,4:7])[1:2]) / sum(colSums(total_mat_glmdr_test[,4:7])) # glmdr
```

```{r, eval=FALSE}
#bayesglm
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  bayesglm_mod <- bayesglm(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(bayesglm_mod, newdata= testing_X, type="response")
  total_mat_bayes_test[i,1] <- i
  total_mat_bayes_test[i,4] <- sum((ifelse(ret2>0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_bayes_test[i,5] <- sum((ifelse(ret2>0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_bayes_test[i,6] <- sum((ifelse(ret2>0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_bayes_test[i,7] <- sum((ifelse(ret2>0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_bayes_test[i,8] <- sum(total_mat_bayes_test[i,4:7])
  total_mat_bayes_test[i,]
}
total_mat_bayes_test <- do.call(rbind,ret)
sum(colSums(total_mat_bayes_test[,4:7])[1:2]) / sum(colSums(total_mat_bayes_test[,4:7])) # bayesglm
```

```{r, eval=FALSE}
#logistf
start_time <- proc.time()
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  logistf_mod <- logistf(Kernel.color~.,data=training, family="binomial")
  new_dat <- rbind(logistf_mod$data,as.data.frame(cbind("Kernel.color"=testing_Y,testing_X),col.names=colnames(logistf_mod$data)))
  ret2 <- c(invlogit(tail(model.matrix(Kernel.color~.,data=new_dat),1) %*% logistf_mod$coefficients))
  total_mat_logistf_test[i,1] <- i
  total_mat_logistf_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_logistf_test[i,5] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_logistf_test[i,6] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_logistf_test[i,7] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_logistf_test[i,8] <- sum(total_mat_logistf_test[i,4:7])
  total_mat_logistf_test[i,]
}
proc.time()-start_time
total_mat_logistf_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
#OLS
ret <- foreach(i=1:n_sample) %dopar% {
  training <- dat[-i,]
  testing_X <- dat[i,-y_idx,drop=FALSE]
  testing_Y <- dat[i,y_idx]
  lm_mod <- lm(Kernel.color~.,data=training, family="binomial")
  ret2 <- predict(lm_mod, newdata= testing_X, type="response")
  total_mat_linear_test[i,1] <- i
  total_mat_linear_test[i,4] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 1)) #true positive
  total_mat_linear_test[i,5] <- sum((ifelse(ret2>=0.5,1,0) == testing_Y) & (testing_Y == 0)) #true negative
  total_mat_linear_test[i,6] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 0)) #false positive
  total_mat_linear_test[i,7] <- sum((ifelse(ret2>=0.5,1,0) != testing_Y) & (testing_Y == 1)) #false negative    
  total_mat_linear_test[i,8] <- sum(total_mat_linear_test[i,4:7])
  total_mat_linear_test[i,]
}
total_mat_linear_test <- do.call(rbind,ret)
```

```{r, eval=FALSE}
sum(colSums(total_mat_glmdr_test[,4:7])[1:2]) / sum(colSums(total_mat_glmdr_test[,4:7])) # glmdr

sum(colSums(total_mat_bayes_test[,4:7])[1:2]) / sum(colSums(total_mat_bayes_test[,4:7])) # bayesglm

sum(colSums(total_mat_logistf_test[,4:7])[1:2]) / sum(colSums(total_mat_logistf_test[,4:7])) # logistf

sum(colSums(total_mat_linear_test[,4:7])[1:2]) / sum(colSums(total_mat_linear_test[,4:7])) # linear
```

