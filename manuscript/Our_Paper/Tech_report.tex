\documentclass[11pt]{extarticle}

\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{import}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{dirtytalk}

\newtheorem{thm}{Theorem}
\newcommand{\Gamlim}{$\Gamma_{\text{lim} \text{ }}$}

\title{\textbf{TITLE}}
\author{Suyoung Park\thanks{Email: spark148@illinois.edu} \text{ }and Daniel Eck\\
\emph{University of Illinois at Urbana-Champaign}}
\date{Month 2020}

\begin{document}
\maketitle
\begin{abstract}
    Space for the Abstract.    
\end{abstract}
\smallskip
\noindent \textbf{Key Words:} list of keywords
\section{Introduction}
\subsection{Background and Literature}

Papers we are going to discuss:

1. \textit{Heinze, 2002, logistf} 

2. \textit{Kosmidis, 2007, brglm2} 

3. \textit{Gelman, 2008, arm (bayesglm)} 

4. \textit{Geyer, 2009, gdor} 

(Possibly we can talk about separation very briefly - Albert A, Anderson JA. On the existence of maximum likelihood estimates in logistic regression models. Biometrika 1984; 71:1â€“10, very first paper talking about the terms, "separation.") (Also Heinze is a good example)
Briefly mention the history of effort to solve the separation problem (when canonical statistic lies on the boundary of its convex support). Explain how each method works.

\subsection{Motivations and Contributions}

Presenting Endometrial example for logistic regression and catrec example for Poisson case (Possibly we want to find "representative (or famous)" example that can be more persuasive). POINT out aforementioned methods (except Geyer's work) used the penalized method and "transform" the problem itself to avoid model-based problem (one of our motivation). 

Introduce Daniel's work and guide reader to see for detail (theoretically).
With Daniel's method, we can achieve 1) the smallest CI (and similar CI comparing to gdor, gdor example has to come to the last so that in the next paragraph I can connect to the lower computational cost) 2) lower computational cost in comparison to rcdd (we need to re-run the test for this case). 3) Canonical statistics are meaningless but only probabilities and expectations $=>$ glmdr provides the mean-value parameters meanwhile other 3 methods provide the canonical statistics (I may need to rewrite this in clearer way).

\section{Problem Formulation}
%%%%% Preliminaries %%%%%
\subsection{Exponential Family}
% Define exponential family
Let X be a random variable or vector with sample space $\mathcal{X} \subset \mathbb{R}^p$ and $\theta$ be a vector parameter with space $\Theta$. An exponential family of probability distributions is a parametric statistical model whose density has the following general form:

\begin{equation}\label{exp_density}
    f_{\theta}(x) = h(x)\exp(\langle T(x),\theta\rangle-c(\theta))
\end{equation}
where $h(x)$ is a underlying measure, $T(x)$ is a vector of sufficient statistics and $c(\theta)$ is the cumulant generating function of the family. $\langle\cdot,\cdot\rangle$ represents the inner product, which is defined as:
$$ \langle T(x), \theta\rangle = \sum_{i=1}^{p} T(x_i) \theta_{i}.$$
% Clarifying the density
The density here can mean either a probability mass function (PMF), a probability density function (PDF), or probability mass-density function (PMDF) depending on the distribution we are referring to.

% Define terms in exp family + some characteristic: fullness and regularity
In the context of general exponential families, the statistics $T(x)$ and parameter $\theta$ are called $\textit{canonical}$ or $\textit{natural}$. An exponential family is said to be $\textit{full}$ if its canonical parameter space is in the effective domain:

\begin{equation}\label{effective_domain}
\Theta = \{\theta : c(\theta) <+\infty \}.
\end{equation}
A full exponential family is said to be $\textit{regular}$ if its canonical parameter space is a non-empty open set.
%% Ques: should I add more explanation on "open set" i.e. notation.

% log likelihood
The log likelihood for the exponential family takes the form: 
\begin{equation}\label{exp_ll}
    l(\theta)=\langle T(x),\theta\rangle-c(\theta)
\end{equation}
Since it is the convention that terms that do not contain the parameter can be dropped from a log likelihood, $h(x)$ in \eqref{exp_density} is dropped while the log density goes to the log likelihood. 

%% It is not smooth to transit to the identifiability.
% Identifiability & Direction of recession and constancy.
%% Ques: Should I make a separate section for this?
Let $Y$ be a canonical statistic in the exponential family (i.e. $Y = T(x)$) and $\theta$, $\psi$ be two different canonical parameters in the space $\Theta$. A exponential family of probability distributions is identifiable if $\theta$ and $\psi$ correspond to a different distribution. An exponential family fails to be identifiable if $\theta$ and $\psi$ correspond to the same distribution. This is equivalent to 
\begin{equation}\label{non-identifiability}
    \langle Y, (\theta - \psi)\rangle = C
\end{equation}
where C is constant. Geometrically, we call this the canonical statistic $Y$ is concentrated on a hyperplane $H = \{y: y^T v = C^{\prime}\}$ for some non-zero vector $v$, which implies some of $Y$ are affine functions of $H$ (i.e. $Y \in H)$. Since both $\theta$ and $\psi$ are in the same parameter space $\Theta$, we can rewrite $\psi$ as the function of $\theta$. Let $\psi = \theta+ sv$ for any scalar $s$. Then, we call any vector $v$ that satisfies \eqref{non-identifiability} a $\textit{direction of constancy}$ of the log likelihood. The set of all directions of constancy is called the $\textit{constancy space}$ of the log likelihood. If $\langle Y, (\theta - \psi)\rangle \leq C$ Then, any vector $v$ that satisfies this called a $\textit{direction of recession}$ of the log likelihood. If every direction of recession is a direction of constancy, then the MLE exists in a full exponential family.

%% Ques: , which implies some components are affine functions of other components 
%% Ques: s or sv?
%% Ques: Direction of rcession.
%% Geyer, 2009, Sections 3.3
%% Rockafellar [Convex Analysis (1970), p.69] Definition of Directions of recession and constancy of convex and concave functions.

\subsection{Mean Value Parameter}

Mean value parameterization has advantages; especially interpretability. we use canonical statistics because they have meaning in the substantive context. The parameter corresponding to each canonical statistic is just its expected value over the population of canonicals. For many parameters, this population-average interpretation has great intuitive appeal. The mean value parameter space is finite and convex, and the points in the space have an interpretable scale.

For the regular full exponential family, the mean-value parameter vector, $\mu$ is defined as:
\begin{equation}\label{Mean_Val_Para}
    \mu = E_{\theta}(y) = \nabla c(\theta).
\end{equation}

The notion of canonical affine submodels of exponentia lfamilies was only propsed recently (Geyer, 2007).

If the model is identifiable, the change-of-parameter from canonical to mean-value parameter is invertible for a regular exponential family. 
In the exponential family, canonical parameters do not represent the probabilities and expectations. In this case, they are not meaningful; we need them because PMF and canononical affine submodels are specified in terms of them. 

We want our interpretations to be in terms of mean-value parameters, since those are the ones directly related to probabilities and expectations.
Mean-value parameters do not change so long as the submodel is the same.

$\hat{\mu} = y$ means the MLE ofr the mean-vlaue parameter is the observed value of the canonical statistc vector $y$. Observed equals expected.

In terms of interpreting MLE, the mena-value parameters are meaningful and the canonical parameters meaningless.

\subsection{Limiting Conditional Model}
For a full exponential family having canonical statistic $Y$, canonical parameter $\theta \in \Theta$, and $H = \{y: y^T v = C\}$ for some constant $C$, and $\text{pr}(Y \in H) > 0$ for some distribution in the family,
\begin{equation}\label{LCM_limit}
    \lim_{s\to\infty} f_{\theta+sv}(x) = \begin{dcases}
    0 & \langle Y, (\theta - \psi)\rangle < C \\
    f_{\theta}(x \text{ } | \text{ } Y \in H) & \langle Y, (\theta - \psi)\rangle = C \\
    +\infty & \langle Y, (\theta - \psi)\rangle > C
    \end{dcases}
\end{equation}

Notice that the family 
\begin{equation}\label{LCM_family}
    \{ f_{\theta}(\cdot \text{ } | \text{ } Y \in H) : \theta \in \Theta \}
\end{equation}
is also an exponential family with the same canonical statistic and parameter with the original family \eqref{exp_density}. We now will call this family the limiting conditional model (LCM). The canonical parameter space of the family is at least
\begin{equation}\label{LCM_space}
    \Theta + \Gamma_{\text{lim}} = \{ \theta + \gamma: \theta \in \Theta \text{ and } \gamma \in \Gamma_{\text{lim}}\},
\end{equation}
where $\Theta$ is the canonical parameter space of the original family and $\Gamma_{\text{lim}}$ is the constancy space of the family \eqref{LCM_family}.
%% Ques: definition of limiting conditional model and its space

The log likelihood for family \eqref{LCM_family} is
$$ l_{\text{LCM}}(\theta) = l(\theta) - \log(\text{pr}_{\theta}(Y \in H))= \langle Y, \theta\rangle - c(\theta) - \log(\text{pr}_{\theta}(Y \in H)).$$
Suppose the MLE exists for the LCM. Then, we call this as a MLE in Barndorff-Nielsen completion of the original family if 1) it maximizes the likelihood in the union of the LCM and the original family, 2) it maximizes the likelihood in the family that that is the set of all limits of sequences of distributions in the original family.
%% Ques: How to insert Theorem 5.
\subsection{Null Space of the Fisher Information}

The null space of the Fisher information matrix (also, the variance-covariance matrix of the canonical statistic or null eigenvector of the Fisher information matrix) is crucial for the statistical inference in our method because we approximate the null space of the Fisher information matrix for an exponential family to find the MLE in the Barndorff-Nielsen completion. Since it is the support of the canonical statistic under the MLE distribution in the completion, it must contain the mean value vector of the canonical statistic. Thus, we can see \Gamlim in \eqref{LCM_space} is the null space of the Fisher information matrix. In our method, we can estimate the null space of the Fisher information matrix from its eigenvalues and eigenvectors using inexact computer arithmetic. 
%% Ques: need to add some description on how to calculate possibly?
%% Ques: relationship between affine hull and null space of the Fisher infromation matrix
%% Ques: what do eigenvalues and vectors tell us?
\begin{comment}
===Ingridient has not been used===

Conventionally, we find the MLE by iteratively going uphill until likelihood function becomes flat.

null space of the Fisher information matrix (FIM) = null eigenvector of the FIM = affine hull
= variance covariance matrix of the canonical statistic. 

We will get nearly the correct affine hull if we can guess the correct null space of the Fisher information matrix from its eigenvalues and eigenvectors computed using inexxact computer arithmetic.

We add theory describing approximate calculation of the MLE in the completion by maximizing the likelihood, showing that such estimates are close to the exact MLE in many respects including moments of all orders, and this allows new methods of calculation based on the null space of the Fisher information matrix. 

- constancy space of the LCM is the \Gamlim
\end{comment}

\subsection{One-Sided Confidence Interval}
%% Add more description on context of one-sided interval
Let $\theta = M \beta$ denote the saturated model canonical parameter (also called \say{linear predictor} in the GLM) where $\beta$ denotes the vector of submodel canonical parameters (often called \say{coefficients} in statistical software) and $M$ denotes a model matrix. Let $\hat{\beta}$ be a MLE in the LCM. Let $I$ denote the index set of the observed values in the response vector on which we condition the original model to get the LCM. Hence, $Y_I$ and $y_I$ denote a random vector of these observed values and its realizations, respectively. Then endpoints for a $100(1-\alpha)\%$ confidence interval for a scalar parameter $g(\beta)$ are
\begin{equation}\label{CI_def}
        \min_{\substack{\gamma \in \Gamma_{\text{lim}} \\ \text{pr}_{\hat{\beta}+\gamma}(Y_{I}=y_{I}) \geq \alpha}}{g(\hat{\beta}+\gamma)} \quad \text{and} \quad \max_{\substack{\gamma \in \Gamma_{\text{lim}} \\ \text{pr}_{\hat{\beta}+\gamma}(Y_{I}=y_{I}) \geq \alpha}}{g(\hat{\beta}+\gamma)}
\end{equation}
where $\Gamma_{\text{lim}}$ is the null space of the Fisher information matrix and $\alpha$ is the significance level. Since we always know if the observed values in the response vector is at the upper or lower of end of its rangem we only need to compute the other end of the confidence interval.

We can see solving \eqref{CI_def} is the optimization problem. In our method, we used the sequential quadratic programming (SQP) method to solve the constrained nonlinear problem.

%% We will be doing confidence intervals for mean value parameters for components of the response vector for which the MLE in the LCM is on the boundary, either zero or one. 

\subsection{Calculating the MLE for discrete GLM}

In a regular full discrete exponential family, the MLE falls into one of three cases; 1) a MLE exists in the original model, 2) a MLE in the Barndorff-Nielsen completion is completely degenerate, and 3) a MLE in the Barndorff-Nielsen completion is not completely degenerate. When the MLE exists in the conventional sense (the first case), we can simply use any statistical software to fit the model. In the latter two cases, we can use our method for statistical inference.

\subsubsection{Completely Degenerate}

Suppose $Y$ is concentrated to some hyperplane $H$, then there exists a non-zero vector $v$ in \eqref{non-identifiability} that $Y$ has no variability. In other words, the variance-covariance matrix for $Y$ is not full rank and the model is not identifiable. If $Y$ repeats to be concentrated to all possible hyperplanes that are nested within each other, the model is completely degenerated. The eigenvalues of the Fisher information then are all zeros and every parameter values $\theta \in \Theta$ corresponds to the same distribution. In this case, every vector of the canonical parameters is an MLE. It implies \Gamlim is the whole parameter space, $\Theta$ and $I$ is the whole index for the reponse vector in \eqref{CI_def}. 
%% Is the variance-covariance matrix = fisher information matrix?
%% (the saturated model MLE's mean value parameters agree with the observed data; they are on the boundary of the set of either zero or one)

\subsubsection{Not Completely Degenerate}

The MLE exists in the Barndorff-Nielsen completion is not completely degenerate, the limiting conditional model conditions on the non-problematic points (linearity = FALSE). 

%%Example for Logistic, Poisson

\subsection{Logistic Regression}

Let $p=\text{logit}^{-1}(\theta)$ denote the mean value parameter vector. Then the probabilities in \eqref{CI_def} are
$$ \text{pr}_{\beta}(Y_{I} = y_{I}) = \prod_{i \in I}{p_{i}^{y_{i}} (1-p_{i})^{n_{i}-y_{i}}} $$
where the $n_{i}$ are the binomial sample sizes. 

We could take the confidence interval problem to be

\begin{equation} \label{CI_logistic}
\begin{split}
    \text{maximize } & \quad p_{k} \\
    \text{subject to } & \quad \prod_{i \in I}{p_{i}^{y_{i}} (1-p_{i})^{n_{i}-y_{i}}} \geq \alpha
\end{split}
\end{equation}
where $p$ is taken to be the function of $\gamma$ described above. And this can be done for any $k \in I$.

Notice that the mean value parameter contains exponential term in the numerator. Thus, when our canonical parameter is extremely large, we will lose precision. Thus, we transform our problems to make it computationally stable. Firstly, we convert maximize problem to minimize problem, optimize canonical parameter rather than mean value parameter and rewrite mean value parameter.

Firstly, solving minimizing problem is much more preferable in the general sense because it is much easier to solve computationally. Secondly, canonical parameter, $\theta_{k} = \text{logit}(p_k)$ is a monotone transformation, so if we find the solution, it will be the solution for the other.
To avoid a catastrophic cancellation, we carefully construct the mean value parameters:
$$p_{i} = \frac{\exp(\theta_{i})}{1+\exp(\theta_{i})} = \frac{1}{1+\exp(-\theta_{i})}$$
$$1-p_{i} = \frac{1}{1+\exp(-\theta_{i})} = \frac{\exp(-\theta_{i})}{1+\exp(-\theta_{i})}$$
Thus, our problem \eqref{CI_logistic} is now

\begin{equation} \label{CI_logistic2}
\begin{split}
    \text{minimize } & \quad -\theta_k \\
    \text{subject to } & \sum_{i \in I}[y_{i} \log{(p_i)} + (n_i - y_i) \log{(1-p_{i}})] - \log{(\alpha)}.
\end{split}
\end{equation}

\begin{comment}
- We minimize canonical rather than mean value parameters to avoid extreme inexactness of computer arithmetic in calculating mean value parameters near zero and one.
\end{comment}

\subsection{Complete Separation}
\subsubsection{Bradley-Terry model}
\subsubsection{Poisson Model}


\section{Solution}
\subsection{Methodology}


\subsubsection{Computational Cost}

\subsection{Analysis and Discussion}
\subsubsection{Results}
\subsubsection{Comparison with other methods}

When comparing the computational cost, check two cases when n increases and p increases and both.


\section{Conclusion}

Covering the drawback in the computational cost as n increases, we is more likely to have non-problematic points and need to iterate more. 

-  (completely degenerate case) Hence, our method is not practically useful because the computational cost is proportional to the number of the sample.


\bibliographystyle{plainnat}
\bibliography{Reference}

\section{Appendix}
Will be correctly labeled later...

\begin{comment}
\section{Idea Note}
This section is for the items will (or will not) be added later but I wrote the first.
-  The $T(x)$ is called a canonical or natural statistics, and $\theta$ is called canonical or natural parameter.
- The canonical statistics, parameter, and cumulant function of an exponential family are not unique.
- Direction of constancy + constancy space?
-  (regularity in exp fam) if \ref{effective_domain} is an open subset of the Euclidean space where $\theta$ takes values.
- Possibly test the computational cost of bayesglm as p increase and n increases.
- model matrix cannot be ill conditioned.

- When there exists a hyperplane $H$ such that $Y$ is concentrated to $H$, then there exists a non-zero direction such that $Y$ exhibits no variability. Therefore, the variance matrix for $Y$ is not full rank and the model is not identifiable. This idea can repeat itself. There can be several hyperplanes, nested within each other, such that the $Y$ is concentrated on each of these hyperplanes. This can continue until we have exhausted the number of dimensions in $Y$. When this occurs, the model is completely generate. The variance matrix is 0 and every parameter value $\theta \in \Theta$ corresponds to the model that generates the observed data with probability one. In other words, all $\theta \in \Theta$ correspond to the same distribution.

- We will not be able to do this when the statistical model has an ill-condiioned model matrix. Ill-conditioning will add spurious nearly zero eigenvalues that arise from the ill-conditioning rather than the concentration of the MLE distribution on the correct affine hull.

- In the example 7.2, 7.3, the MLE distribution is only partially but not completely degenerate. This foloows from the estimated Fisher information matrix being signular (to within the accuracy of computer arithmetic) but not the zero matrix. Some of its eigenvalues are zero, butt not all of them. The MLE of some of the saturated model mean value parameters agree with the observed data, but not all. 

- the limiting distribution evalulated along the iterates of a likelihood maximizing sequence has log density that is a generalized affine function with structure given by Theorem 5. Cumulant generating functions coverge alogn this sequence of iterate (Theorems 6 and 7), do estimates of moments of all order (Theorem 10) for distributions takign estimated parameter valuyes alogn this sequence of iterates.

- (mean-value parameter)

For a regular full exponential family,
use (here formula 5) to define:

$$\mu = E_{\theta}(y) = \Delta c(\theta).$$

* i.e Gradient (or Jacobian) of cumulant generating function
The vector $\mu$ is called the mean-value parameter vector.

relevant theorem:
The mean value parameterization of a regular full exponential family is always identifiable: every distribution in the family has a mean vector, and differnt distributions have different mean vectors.

(Review, Barndorff-Nielsen 1978, p121).

Remark: 

$$\mu_1 = \Delta c(\theta_{1})$$
$$\mu_2 = \Delta c(\theta_{2})$$
and $\mu_1 = \mu_2$, then $\theta_{1}-\theta_{2}$ is a direction of constancy.

Canonical statistic of the submodel is $M^T y$, so its mean has to be $M^T \mu$ by linearity of expectation.

$$\theta = \alpha + M \beta$$
$$ \tau = M^T \mu$$ 
* $\tau$ is submodel mean-value parameter.

- For a full exponnetial family with convex support $C$ and observed value of the canonical statistic $y$ such that $y \in C$, 
The MLE exists = Every direction of recession is a direction of constancy.

* Every direction of constancy is a direction of recession but convserse is not true.

- By Geyer pg 12 corollary 2, all maximum likelihood estimates correspond to the same distribution. Thus we have uniquenss.

- every vector is an MLE. We take the zero vector to be $\hat{\beta}.$ LEt $I$ denote the index set of the components of the response vector on which we condition the original model (OM) to get the limiting conditional model (LCM). So, $I$ is the whole index vector for the model. Let $Y_{I}$ and $y_{I}$ denote the the corresponding components of the response vector considered as a random vector and as an observed value, respectively. Then, we can calculate the confidence interval using \eqref{CI_def} where $\Gamma_{\text{lim}}$ is the constancy space of the LCM. In this example, 
\Gamlim is the whole parameter space. In these expressions $\text{pr}$ denotes probability with respect to the OM not the LCM. 


\end{comment}



\end{document}